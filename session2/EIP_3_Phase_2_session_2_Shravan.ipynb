{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP 3 Phase 2 session 2 Shravan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lRbm2r__pwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import sys\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQFxRQxonNr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try:\n",
        "#     device_name = os.environ['COLAB_TPU_ADDR']\n",
        "#     TPU_ADDRESS = 'grpc://' + device_name\n",
        "#     print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "# except KeyError:\n",
        "#     print('TPU not found')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtKLvwcq_5oL",
        "colab_type": "code",
        "outputId": "eba2ef0d-30cc-40a3-a370-4b0a9d1e1595",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploadedFile = files.upload()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a0d95b4-c79c-4005-9d54-682964f88285\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1a0d95b4-c79c-4005-9d54-682964f88285\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving alice.txt to alice (1).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x89859HUAmHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"alice.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "#remove punctuation\n",
        "raw_text = re.sub(r'[^\\w\\s]','',raw_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68SiXfrbA3MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1sAyAmUBAtH",
        "colab_type": "code",
        "outputId": "5128c6bb-39f7-4946-d164-f5661ad0ccd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136186\n",
            "Total Vocab:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2eTO9aKHnK0",
        "colab_type": "code",
        "outputId": "66308258-ed08-4793-9a50-33fd6ba40021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "#adding padding sequence  \n",
        "dataXpad = pad_sequences(dataX, maxlen=seq_length)  \n",
        "n_patterns = len(dataXpad)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  136086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBrPZJCNHyIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5TLDuWEIi7B",
        "colab_type": "code",
        "outputId": "53fcbb4c-bb00-499b-999c-5e1c61c950f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True,dropout=0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 17:53:33.475324 140635696752512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 17:53:33.494093 140635696752512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 17:53:33.496808 140635696752512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 17:53:33.658560 140635696752512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0721 17:53:33.668927 140635696752512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 31)                7967      \n",
            "=================================================================\n",
            "Total params: 797,471\n",
            "Trainable params: 797,471\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQDaE9GmI5E8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a9bf8275-46c7-40f2-ea87-a958511bb16a"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 17:53:34.193724 140635696752512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 17:53:34.220710 140635696752512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9g5JCemqvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def model_to_tpu(model):\n",
        "#    return tf.contrib.tpu.keras_to_tpu_model( model, strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MBi8NTPnjgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tpu_model = model_to_tpu(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avAr8WkrJ9vv",
        "colab_type": "code",
        "outputId": "89c32c73-8406-445b-844d-cb38cd8cbf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 17:53:34.363873 140635696752512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "136086/136086 [==============================] - 291s 2ms/step - loss: 2.7347\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.73474, saving model to weights-improvement-01-2.7347.hdf5\n",
            "Epoch 2/100\n",
            "136086/136086 [==============================] - 282s 2ms/step - loss: 2.3614\n",
            "\n",
            "Epoch 00002: loss improved from 2.73474 to 2.36143, saving model to weights-improvement-02-2.3614.hdf5\n",
            "Epoch 3/100\n",
            "136086/136086 [==============================] - 282s 2ms/step - loss: 2.1475\n",
            "\n",
            "Epoch 00003: loss improved from 2.36143 to 2.14750, saving model to weights-improvement-03-2.1475.hdf5\n",
            "Epoch 4/100\n",
            "136086/136086 [==============================] - 282s 2ms/step - loss: 2.0021\n",
            "\n",
            "Epoch 00004: loss improved from 2.14750 to 2.00214, saving model to weights-improvement-04-2.0021.hdf5\n",
            "Epoch 5/100\n",
            "136086/136086 [==============================] - 280s 2ms/step - loss: 1.8726\n",
            "\n",
            "Epoch 00005: loss improved from 2.00214 to 1.87259, saving model to weights-improvement-05-1.8726.hdf5\n",
            "Epoch 6/100\n",
            "136086/136086 [==============================] - 278s 2ms/step - loss: 1.9235\n",
            "\n",
            "Epoch 00006: loss did not improve from 1.87259\n",
            "Epoch 7/100\n",
            "136086/136086 [==============================] - 277s 2ms/step - loss: 1.8219\n",
            "\n",
            "Epoch 00007: loss improved from 1.87259 to 1.82193, saving model to weights-improvement-07-1.8219.hdf5\n",
            "Epoch 8/100\n",
            "136086/136086 [==============================] - 276s 2ms/step - loss: 1.7576\n",
            "\n",
            "Epoch 00008: loss improved from 1.82193 to 1.75762, saving model to weights-improvement-08-1.7576.hdf5\n",
            "Epoch 9/100\n",
            "136086/136086 [==============================] - 276s 2ms/step - loss: 1.7051\n",
            "\n",
            "Epoch 00009: loss improved from 1.75762 to 1.70515, saving model to weights-improvement-09-1.7051.hdf5\n",
            "Epoch 10/100\n",
            "136086/136086 [==============================] - 276s 2ms/step - loss: 1.6578\n",
            "\n",
            "Epoch 00010: loss improved from 1.70515 to 1.65784, saving model to weights-improvement-10-1.6578.hdf5\n",
            "Epoch 11/100\n",
            "136086/136086 [==============================] - 276s 2ms/step - loss: 1.6158\n",
            "\n",
            "Epoch 00011: loss improved from 1.65784 to 1.61578, saving model to weights-improvement-11-1.6158.hdf5\n",
            "Epoch 12/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 1.5783\n",
            "\n",
            "Epoch 00012: loss improved from 1.61578 to 1.57831, saving model to weights-improvement-12-1.5783.hdf5\n",
            "Epoch 13/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 1.5410\n",
            "\n",
            "Epoch 00013: loss improved from 1.57831 to 1.54105, saving model to weights-improvement-13-1.5410.hdf5\n",
            "Epoch 14/100\n",
            "136086/136086 [==============================] - 270s 2ms/step - loss: 1.5079\n",
            "\n",
            "Epoch 00014: loss improved from 1.54105 to 1.50793, saving model to weights-improvement-14-1.5079.hdf5\n",
            "Epoch 15/100\n",
            "136086/136086 [==============================] - 271s 2ms/step - loss: 1.4750\n",
            "\n",
            "Epoch 00015: loss improved from 1.50793 to 1.47499, saving model to weights-improvement-15-1.4750.hdf5\n",
            "Epoch 16/100\n",
            "136086/136086 [==============================] - 271s 2ms/step - loss: 1.4470\n",
            "\n",
            "Epoch 00016: loss improved from 1.47499 to 1.44704, saving model to weights-improvement-16-1.4470.hdf5\n",
            "Epoch 17/100\n",
            "136086/136086 [==============================] - 270s 2ms/step - loss: 1.4151\n",
            "\n",
            "Epoch 00017: loss improved from 1.44704 to 1.41506, saving model to weights-improvement-17-1.4151.hdf5\n",
            "Epoch 18/100\n",
            "136086/136086 [==============================] - 273s 2ms/step - loss: 1.3892\n",
            "\n",
            "Epoch 00018: loss improved from 1.41506 to 1.38923, saving model to weights-improvement-18-1.3892.hdf5\n",
            "Epoch 19/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 1.3682\n",
            "\n",
            "Epoch 00019: loss improved from 1.38923 to 1.36817, saving model to weights-improvement-19-1.3682.hdf5\n",
            "Epoch 20/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 1.3396\n",
            "\n",
            "Epoch 00020: loss improved from 1.36817 to 1.33959, saving model to weights-improvement-20-1.3396.hdf5\n",
            "Epoch 21/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 1.3202\n",
            "\n",
            "Epoch 00021: loss improved from 1.33959 to 1.32021, saving model to weights-improvement-21-1.3202.hdf5\n",
            "Epoch 22/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 1.2975\n",
            "\n",
            "Epoch 00022: loss improved from 1.32021 to 1.29754, saving model to weights-improvement-22-1.2975.hdf5\n",
            "Epoch 23/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 1.8569\n",
            "\n",
            "Epoch 00023: loss did not improve from 1.29754\n",
            "Epoch 24/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.8074\n",
            "\n",
            "Epoch 00024: loss did not improve from 1.29754\n",
            "Epoch 25/100\n",
            "136086/136086 [==============================] - 276s 2ms/step - loss: 2.6971\n",
            "\n",
            "Epoch 00025: loss did not improve from 1.29754\n",
            "Epoch 26/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.6350\n",
            "\n",
            "Epoch 00026: loss did not improve from 1.29754\n",
            "Epoch 27/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.5967\n",
            "\n",
            "Epoch 00027: loss did not improve from 1.29754\n",
            "Epoch 28/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.5636\n",
            "\n",
            "Epoch 00028: loss did not improve from 1.29754\n",
            "Epoch 29/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.5333\n",
            "\n",
            "Epoch 00029: loss did not improve from 1.29754\n",
            "Epoch 30/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.5052\n",
            "\n",
            "Epoch 00030: loss did not improve from 1.29754\n",
            "Epoch 31/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.4830\n",
            "\n",
            "Epoch 00031: loss did not improve from 1.29754\n",
            "Epoch 32/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.4653\n",
            "\n",
            "Epoch 00032: loss did not improve from 1.29754\n",
            "Epoch 33/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.4490\n",
            "\n",
            "Epoch 00033: loss did not improve from 1.29754\n",
            "Epoch 34/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.4333\n",
            "\n",
            "Epoch 00034: loss did not improve from 1.29754\n",
            "Epoch 35/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.4193\n",
            "\n",
            "Epoch 00035: loss did not improve from 1.29754\n",
            "Epoch 36/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.4053\n",
            "\n",
            "Epoch 00036: loss did not improve from 1.29754\n",
            "Epoch 37/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.3909\n",
            "\n",
            "Epoch 00037: loss did not improve from 1.29754\n",
            "Epoch 38/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.3779\n",
            "\n",
            "Epoch 00038: loss did not improve from 1.29754\n",
            "Epoch 39/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3688\n",
            "\n",
            "Epoch 00039: loss did not improve from 1.29754\n",
            "Epoch 40/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3819\n",
            "\n",
            "Epoch 00040: loss did not improve from 1.29754\n",
            "Epoch 41/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3471\n",
            "\n",
            "Epoch 00041: loss did not improve from 1.29754\n",
            "Epoch 42/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3370\n",
            "\n",
            "Epoch 00042: loss did not improve from 1.29754\n",
            "Epoch 43/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3264\n",
            "\n",
            "Epoch 00043: loss did not improve from 1.29754\n",
            "Epoch 44/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.3174\n",
            "\n",
            "Epoch 00044: loss did not improve from 1.29754\n",
            "Epoch 45/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3097\n",
            "\n",
            "Epoch 00045: loss did not improve from 1.29754\n",
            "Epoch 46/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.3015\n",
            "\n",
            "Epoch 00046: loss did not improve from 1.29754\n",
            "Epoch 47/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.2934\n",
            "\n",
            "Epoch 00047: loss did not improve from 1.29754\n",
            "Epoch 48/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.2839\n",
            "\n",
            "Epoch 00048: loss did not improve from 1.29754\n",
            "Epoch 49/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.2749\n",
            "\n",
            "Epoch 00049: loss did not improve from 1.29754\n",
            "Epoch 50/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.2680\n",
            "\n",
            "Epoch 00050: loss did not improve from 1.29754\n",
            "Epoch 51/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.2599\n",
            "\n",
            "Epoch 00051: loss did not improve from 1.29754\n",
            "Epoch 52/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.2539\n",
            "\n",
            "Epoch 00052: loss did not improve from 1.29754\n",
            "Epoch 53/100\n",
            "136086/136086 [==============================] - 275s 2ms/step - loss: 2.2444\n",
            "\n",
            "Epoch 00053: loss did not improve from 1.29754\n",
            "Epoch 54/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.2379\n",
            "\n",
            "Epoch 00054: loss did not improve from 1.29754\n",
            "Epoch 55/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.2317\n",
            "\n",
            "Epoch 00055: loss did not improve from 1.29754\n",
            "Epoch 56/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.2265\n",
            "\n",
            "Epoch 00056: loss did not improve from 1.29754\n",
            "Epoch 57/100\n",
            "136086/136086 [==============================] - 274s 2ms/step - loss: 2.2211\n",
            "\n",
            "Epoch 00057: loss did not improve from 1.29754\n",
            "Epoch 58/100\n",
            " 62976/136086 [============>.................] - ETA: 2:27 - loss: 2.2133Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbBUT7mDMmMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5bfbc89c-29fb-429e-f177-ba0cba971a6a"
      },
      "source": [
        "\n",
        "# load the network weights\n",
        "filename = \"weights-improvement-22-1.2975.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" could think of\n",
            "nothing better to say than his first remark it was the best butter\n",
            "you know\n",
            "\n",
            "alice ha \"\n",
            "d noleny toeezed uound tound oore of thrpled anlce whought lt herply intertupted inl gorugh hir soease yhen i had tounded tounde of tuch a louder tfparked tuite a little bnd taid to hereratly ano the sepembered tooe off tipgictly ano toeed oo a little bnd toutently ano toeeze sfplied anlce whought ierself sound and serpee onrsed up the whought souw \n",
            "iere taid the halled herself anl toeeze sfparked tound to whe tteen onse asd anlce whoue tourd thedhe ano toeed tourd the whe qther sffes one hldut\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}