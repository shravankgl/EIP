{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNST_CIFAR10_Shravan_B9_CLR_93.03.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "K70hAckqg0EA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # https://keras.io/\n",
        "# !pip install -q keras\n",
        "# import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, Activation,GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint,ReduceLROnPlateau\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5uLi6C9pArQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Copied code from https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py\n",
        "from tensorflow.keras.callbacks import *\n",
        "class CyclicLR(Callback):\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UNHw6luQg3gc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# # backend\n",
        "# import tensorflow as tf\n",
        "# from keras import backend as k\n",
        "\n",
        "# # Don't pre-allocate memory; allocate as-needed\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "\n",
        "# # Create a session with the above options specified.\n",
        "# k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dsO_yGxcg5D8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 250\n",
        "l = 16\n",
        "num_filter = 32\n",
        "growth_rate = 12\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "weight_decay = 1e-4\n",
        "dilate_rate = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d5f92462-8c11-4fce-9479-2ab7d9a40174"
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "for i in range(3):\n",
        "\t\tmean = np.mean(x_train[:, :, :, i])\n",
        "\t\tstd = np.std(x_train[:, :, :, i])\n",
        "\t\tx_train[:, :, :, i] = (x_train[:, :, :, i] - mean) / std\n",
        "\t\tx_test[:, :, :, i] = (x_test[:, :, :, i] - mean) / std"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 48s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ee-sge5Kg7vr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression, weight_decay, growth_rate\n",
        "\n",
        "    temp = input\n",
        "    #concat_layers = [input]\n",
        "    for _ in range(l):\n",
        "      \n",
        "        BatchNorm = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_1_1 = Conv2D(int(growth_rate*4), (1,1), use_bias=False ,padding='same',\n",
        "                              kernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(relu)\n",
        "        \n",
        "        BatchNorm_1_1 = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(Conv2D_1_1)\n",
        "        relu_1_1 = Activation('relu')(BatchNorm_1_1)\n",
        "        Conv2D_3_3 = Conv2D(int(growth_rate), (3,3), use_bias=False ,padding='same', dilation_rate = dilate_rate,\n",
        "                              kernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(relu_1_1)\n",
        "        \n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(rate=dropout_rate)(Conv2D_3_3)\n",
        "        #concat_layers.append(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        num_filter += growth_rate\n",
        "        \n",
        "    return temp , num_filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OOP6IPsGhBwb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression, weight_decay\n",
        "    BatchNorm = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same',\n",
        "                              kernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(rate=dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2),strides=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0RaKFpubhDIC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_layer(input):\n",
        "    global compression, weight_decay\n",
        "    BatchNorm = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    #AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    #flat = Flatten()(AvgPooling)\n",
        "    #output = Dense(num_classes, activation='softmax')(flat)\n",
        "    GloAvgPooling = GlobalAveragePooling2D()(relu)\n",
        "    output = Dense(num_classes, activation='softmax',\n",
        "\t\tkernel_regularizer=l2(weight_decay),bias_regularizer=l2(weight_decay))(GloAvgPooling)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "anPCpQWhhGb7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input = Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same', dilation_rate = dilate_rate,\n",
        "\t\t\tkernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(input)\n",
        "\n",
        "First_Block, num_filters = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filters, dropout_rate)\n",
        "\n",
        "Second_Block,num_filters = add_denseblock(First_Transition, num_filters, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filters, dropout_rate)\n",
        "\n",
        "# Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "# Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block,num_filters = add_denseblock(Second_Transition,  num_filters, dropout_rate)\n",
        "output = output_layer(Last_Block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1kFh7pdxhNtT",
        "outputId": "a7e21f89-f881-40ae-caca-0050d8ef6463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14552
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 48)   1536        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 48)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   5184        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 44)   0           conv2d[0][0]                     \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 44)   176         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 44)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 48)   2112        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 56)   0           concatenate[0][0]                \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 56)   224         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 56)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 48)   2688        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   5184        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 68)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 68)   272         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 68)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 48)   3264        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 48)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 48)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   5184        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 80)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 80)   320         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 48)   3840        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 48)   192         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 48)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   5184        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 92)   0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 92)   368         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 92)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 48)   4416        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 48)   192         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 48)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   5184        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 104)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 104)  416         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 104)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 48)   4992        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 48)   192         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 12)   5184        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 116)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 116)  464         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 116)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 48)   5568        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 48)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 48)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 12)   5184        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 128)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 128)  512         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 48)   6144        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 48)   192         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 48)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 12)   5184        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 140)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 32, 32, 140)  560         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 140)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 32, 32, 48)   6720        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 32, 32, 48)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 48)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 12)   5184        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 152)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 32, 32, 152)  608         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 152)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 32, 32, 48)   7296        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 32, 32, 48)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 48)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 32, 32, 12)   5184        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 164)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 32, 32, 164)  656         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 32, 32, 164)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 32, 32, 48)   7872        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 32, 32, 48)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 32, 32, 48)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 12)   5184        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 176)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 176)  704         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 176)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 48)   8448        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 48)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 48)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 32, 32, 12)   5184        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 188)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 32, 32, 188)  752         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 32, 32, 188)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 32, 32, 48)   9024        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 32, 32, 48)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 32, 32, 48)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 12)   5184        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 32, 32, 200)  0           concatenate_12[0][0]             \n",
            "                                                                 dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 32, 32, 200)  800         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 200)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 32, 32, 48)   9600        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 32, 32, 48)   192         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 32, 32, 48)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 32, 32, 12)   5184        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 32, 32, 212)  0           concatenate_13[0][0]             \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 32, 32, 212)  848         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 32, 32, 212)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 32, 48)   10176       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 32, 48)   192         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 32, 32, 48)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 12)   5184        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 32, 32, 12)   0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 32, 32, 224)  0           concatenate_14[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 224)  896         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 32, 32, 224)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 112)  25088       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 32, 32, 112)  0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 112)  0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 16, 16, 112)  448         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 112)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 48)   5376        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 48)   192         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 48)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 12)   5184        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 124)  0           average_pooling2d[0][0]          \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 124)  496         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 124)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 48)   5952        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 48)   192         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 48)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 12)   5184        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 136)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 136)  544         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 136)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 48)   6528        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 48)   192         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 48)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 12)   5184        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 148)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 148)  592         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 148)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 48)   7104        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 48)   192         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 48)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 16, 16, 12)   5184        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 160)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 160)  640         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 160)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 48)   7680        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 48)   192         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 48)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 12)   5184        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 172)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 172)  688         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 172)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 48)   8256        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 48)   192         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 48)   0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 12)   5184        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 184)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 184)  736         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 184)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 48)   8832        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 48)   192         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 48)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 12)   5184        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 196)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 196)  784         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 196)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 48)   9408        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 48)   192         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 48)   0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 12)   5184        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 208)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 16, 16, 208)  832         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 208)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 48)   9984        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 16, 16, 48)   192         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 16, 16, 48)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 12)   5184        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 220)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 16, 16, 220)  880         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 16, 16, 220)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 16, 16, 48)   10560       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 16, 16, 48)   192         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 16, 16, 48)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 16, 16, 12)   5184        activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 12)   0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 16, 16, 232)  0           concatenate_24[0][0]             \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 16, 16, 232)  928         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 16, 16, 232)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 16, 16, 48)   11136       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 16, 16, 48)   192         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 16, 16, 48)   0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 12)   5184        activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 16, 16, 12)   0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 16, 16, 244)  0           concatenate_25[0][0]             \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 16, 16, 244)  976         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 16, 16, 244)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 16, 16, 48)   11712       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 16, 16, 48)   192         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 16, 16, 48)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 16, 16, 12)   5184        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 16, 16, 12)   0           conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 16, 16, 256)  0           concatenate_26[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 16, 16, 256)  1024        concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 16, 16, 256)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 16, 16, 48)   12288       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 16, 16, 48)   192         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 16, 16, 48)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 16, 16, 12)   5184        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 16, 16, 12)   0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 16, 16, 268)  0           concatenate_27[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 16, 16, 268)  1072        concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 16, 16, 268)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 16, 16, 48)   12864       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 16, 16, 48)   192         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 16, 16, 48)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 16, 16, 12)   5184        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 16, 16, 12)   0           conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 16, 16, 280)  0           concatenate_28[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 16, 16, 280)  1120        concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 16, 16, 280)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 16, 16, 48)   13440       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 16, 16, 48)   192         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 16, 16, 48)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 16, 16, 12)   5184        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 16, 16, 12)   0           conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 16, 16, 292)  0           concatenate_29[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 16, 16, 292)  1168        concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 16, 16, 292)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 16, 16, 48)   14016       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 16, 16, 48)   192         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 16, 16, 48)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 16, 16, 12)   5184        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 16, 16, 12)   0           conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 16, 16, 304)  0           concatenate_30[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 16, 16, 304)  1216        concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 16, 16, 304)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 16, 16, 208)  63232       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 16, 16, 208)  0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 208)    0           dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 8, 8, 208)    832         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 8, 8, 208)    0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 8, 8, 48)     9984        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 48)     192         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 48)     0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 12)     5184        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 12)     0           conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 220)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 220)    880         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 220)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 48)     10560       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 48)     192         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 48)     0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 12)     5184        activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 12)     0           conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 232)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 232)    928         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 232)    0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 48)     11136       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 48)     192         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 48)     0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 12)     5184        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 12)     0           conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 244)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 244)    976         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 244)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 48)     11712       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 48)     192         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 48)     0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 12)     5184        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 12)     0           conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 256)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 256)    1024        concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 256)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 48)     12288       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 48)     192         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 48)     0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 12)     5184        activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 12)     0           conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 268)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 268)    1072        concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 268)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 48)     12864       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 48)     192         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 48)     0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 12)     5184        activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 12)     0           conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 8, 8, 280)    0           concatenate_36[0][0]             \n",
            "                                                                 dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 280)    1120        concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 280)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 48)     13440       activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 48)     192         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 48)     0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 12)     5184        activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 8, 8, 12)     0           conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 8, 8, 292)    0           concatenate_37[0][0]             \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 292)    1168        concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 292)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 48)     14016       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 48)     192         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 48)     0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 12)     5184        activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 8, 8, 12)     0           conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 8, 8, 304)    0           concatenate_38[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 304)    1216        concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 304)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 48)     14592       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 48)     192         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 48)     0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 12)     5184        activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 8, 8, 12)     0           conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 8, 8, 316)    0           concatenate_39[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 316)    1264        concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 316)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 48)     15168       activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 48)     192         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 48)     0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 12)     5184        activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 8, 8, 12)     0           conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 8, 8, 328)    0           concatenate_40[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 8, 8, 328)    1312        concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 328)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 48)     15744       activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 48)     192         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 48)     0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 12)     5184        activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 8, 8, 12)     0           conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 8, 8, 340)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 340)    1360        concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 340)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 48)     16320       activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 48)     192         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 48)     0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 12)     5184        activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 8, 8, 12)     0           conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 8, 8, 352)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 352)    1408        concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 352)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 48)     16896       activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 48)     192         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 48)     0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 12)     5184        activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 8, 8, 12)     0           conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 8, 8, 364)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 364)    1456        concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 364)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 48)     17472       activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 48)     192         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 48)     0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 8, 8, 12)     5184        activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 8, 8, 12)     0           conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 8, 8, 376)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 8, 8, 376)    1504        concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 8, 8, 376)    0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 8, 8, 48)     18048       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 8, 8, 48)     192         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 8, 8, 48)     0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 8, 8, 12)     5184        activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 8, 8, 12)     0           conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 8, 8, 388)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 8, 8, 388)    1552        concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 8, 8, 388)    0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 8, 8, 48)     18624       activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 8, 8, 48)     192         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 8, 8, 48)     0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 8, 8, 12)     5184        activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 8, 8, 12)     0           conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 8, 8, 400)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 8, 8, 400)    1600        concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 8, 8, 400)    0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 400)          0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           4010        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 872,458\n",
            "Trainable params: 846,090\n",
            "Non-trainable params: 26,368\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IeZElodLEG45",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "\t\t\tfeaturewise_center=False,  # set input mean to 0 over the dataset\n",
        "\t\t\tsamplewise_center=False,  # set each sample mean to 0\n",
        "\t\t\tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "\t\t\tsamplewise_std_normalization=False,  # divide each input by its std\n",
        "\t\t\tzca_whitening=False,  # apply ZCA whitening\n",
        "\t\t\trotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "\t\t\twidth_shift_range=0.16,  # randomly shift images horizontally (fraction of total width)\n",
        "\t\t\theight_shift_range=0.16,  # randomly shift images vertically (fraction of total height)\n",
        "\t\t\thorizontal_flip=True,  # randomly flip images\n",
        "\t\t\tvertical_flip=False) # randomly flip images\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l0CI3oZKvqyQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def step_decay(epoch):\n",
        "\tinitial_lrate = 0.1\n",
        "\tlrate = 0.1\n",
        "\tif epoch >= 125 and epoch < 187:\n",
        "\t\tlrate = initial_lrate / 10\n",
        "\tif epoch >= 187 :\n",
        "\t\tlrate = initial_lrate / 100\n",
        "\t\n",
        "\treturn float(lrate)\n",
        "lrschedular = LearningRateScheduler(step_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jqXjjnixLtKn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
        "                                    cooldown=0, patience=5, min_lr=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DFlWLDk_m5H3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf clr_callback.py*\n",
        "# !wget https://github.com/bckenstler/CLR/raw/master/clr_callback.py\n",
        "# from clr_callback import *\n",
        "\n",
        "clr_triangular = CyclicLR(mode='triangular',base_lr=0.001, max_lr=0.1, step_size=782*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "b4XOsW3ahSkL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=Adam(),\n",
        "#               metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(),#momentum=0.9,nesterov=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5oLYwEMqK79",
        "outputId": "8307c802-5113-4dde-c813-7ac2dc40228b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "                                              strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                                                  tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://'+os.environ['COLAB_TPU_ADDR'])\n",
        "                                              )\n",
        "                                             )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.88.154.26:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1020327298364076045)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16823090583848632087)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 8288105228803745245)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2081149489486390110)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 17831895952663696395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10349254179339134059)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7688685676268971244)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16201659910051485258)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10021124734368508498)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10873655799616862938)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12708086314670161758)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 10917588067324454807)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "crhGk7kEhXAz",
        "outputId": "15133c7c-476a-451b-fc92-f83ed27f0637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7398
        }
      },
      "cell_type": "code",
      "source": [
        "# model.fit(x_train, y_train,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=50,\n",
        "#                     verbose=1,\n",
        "#                     validation_data=(x_test, y_test),\n",
        "#                     callbacks=[clr_triangular])\n",
        "\n",
        "tpu_model.fit_generator(datagen.flow(x_train, y_train,\n",
        "\t\t\t\t\t\t\t\t\t batch_size=64),\n",
        "\t\t\t\t\t\tsteps_per_epoch=782,epochs=200,validation_data=(x_test, y_test),verbose=1,callbacks=[clr_triangular])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(8, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(8, 10), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.0010000000474974513, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7efd567d5828> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 92.14980912208557 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU momentum: 0.0 {0.0}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "  2/782 [..............................] - ETA: 27:26:06 - loss: 4.4977 - acc: 0.1172WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (8.269438). Check your callbacks.\n",
            " 84/782 [==>...........................] - ETA: 35:58 - loss: 4.3455 - acc: 0.1138INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(2,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(2, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(2, 10), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7efd567d5828> [<tf.Variable 'tpu_139626645559336/SGD/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b7c4198>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b7c4668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b7c4828>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b7c4e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b799cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b764fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b70df98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b6f8cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b643cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b667d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b5d57f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b5a2fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b544e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b534550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b4fce48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b49c470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b40cc50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b3d7fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b3a0a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b30fd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b3364e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b2fde80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b269fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b210cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b1d9da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b146ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b110c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b136f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b0a1d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b06add8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3b013ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3affecf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3af47eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3af6ce48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3aedbd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3aea5d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ae4afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ae35e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3adffdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ada5cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ad13f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3acddf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ad02dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ac6ec50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ac3bd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3abde400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ab4bc18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3ab14f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3aade278>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3aa4d748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3aa71ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3aa39e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a9a7f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a94a160>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a918fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a902ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a8a8f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a873eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a7defd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a7a8c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a750a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a73be48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a685ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a6aadd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a61bc18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a5e1eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a588f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a574da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a53dd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a4e2c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a450d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a419d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a43bdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a3acc18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a376fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a31ccc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a287b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a250ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a218208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a188a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a1acfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a177b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a0e4ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a08d0f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a057908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3a040e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39fe3d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39fb0e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39f1ff28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39ee9c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39e8ceb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39e77dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39e40e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39de5d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39d53ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39d1dd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39cc4ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39cafcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39c79cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39c22b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39b8ccf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39b52cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39b7bd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39aeb7b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39ab6d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39a56e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd399c5588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3998fe80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd399b4b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39920f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd398eaf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd398b3ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39822d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd397c8d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39791eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3977efd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39722cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd396ebdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3965bf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39624ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd395c6e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd395b5d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3957ce10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39526cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39492d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3945bef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39480e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd393ecfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd393b6d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3935ff60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd392c8e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39293e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd392b8cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39228f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd391eff98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39195e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39182550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd390ccd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd390eecc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3905fc18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd39028f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38ff02b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38f5f780>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38f824a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38ecce48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38ebbf98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38e61198>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38e2a4a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38d99f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38dbcf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38d05ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38cf3cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38cbbda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38c63eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38bd0cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38b99f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38bbfe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38b2fd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38af7ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38a9bf98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38a05da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd389d3da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd389f3c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38964f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3892ffd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd388d1da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38843c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3880dcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38832cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3879eba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38766f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3872e240>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3869de80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd386c2d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3860bb70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd385faf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd385a1128>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3856a940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd384d7eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd384f8d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38445e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3842ec88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd383fdf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3839fc18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3830de10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd382d5eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd382fcda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3826bf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38233ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd381dbf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38146d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38111d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd38135c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd380a2d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3806ccf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3800fda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd380017f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37f4bd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37f6ec88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37edb5f8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37ea2eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37e721d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37e37f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37e00f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37d4ab00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37d36be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37cde978>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37ca8f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37c13fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37c39d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37c00e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37b72c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37b3df60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37adee80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37a4eda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37a16e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37a3ad30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd379a8c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37972f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37916eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37902cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3784dfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37875f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd377e0c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd377a9e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3774cd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3773e7b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37687fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd376abe48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3761a550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd375e4e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37587470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37575c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37540fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3748ba90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37479d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3741d4e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd373e4e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37352fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37378cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37341da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd372aeef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37279c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3721cf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37188d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37152dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3717def0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd370e6cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd370afeb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd37055e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36fc4d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36f8ed30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36fb0fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36f1ee10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36ee6dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36e8dcc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36e7df28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36dc4f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36de9dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36d58c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36d22d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36cc7400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36cb4c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36c80f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36bc5278>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36bb7748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36b58ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36b23e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36a90f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36ab5160>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36a7ffd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd369ebef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3698ff28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3695beb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd368c5fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36891c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd368baa20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36824e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd367eeef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36792dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36704c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd366ceeb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd366f0f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3665dda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36627d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd365c9c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd365b8d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36582d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36528dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd364eecc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36461fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36406cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd363f1b70>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd363baef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36382208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd362f4a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd3629afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36261b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7efd36205ba8>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 96.71186065673828 secs\n",
            "781/782 [============================>.] - ETA: 0s - loss: 3.9764 - acc: 0.2506INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(8, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(8, 10), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.0010000000474974513, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7efd199d93c8> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 72.33573961257935 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(2,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(2, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(2, 10), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7efd199d93c8> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 45.660088539123535 secs\n",
            "782/782 [==============================] - 662s 847ms/step - loss: 3.9760 - acc: 0.2507 - val_loss: 3.5809 - val_acc: 0.3864\n",
            "Epoch 2/200\n",
            "782/782 [==============================] - 70s 89ms/step - loss: 3.5947 - acc: 0.3901 - val_loss: 3.6030 - val_acc: 0.4235\n",
            "Epoch 3/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 3.3209 - acc: 0.4877 - val_loss: 3.2046 - val_acc: 0.5398\n",
            "Epoch 4/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 3.1426 - acc: 0.5448 - val_loss: 3.0532 - val_acc: 0.5922\n",
            "Epoch 5/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 3.1078 - acc: 0.5553 - val_loss: 3.3031 - val_acc: 0.5440\n",
            "Epoch 6/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 3.0867 - acc: 0.5543 - val_loss: 2.9125 - val_acc: 0.6159\n",
            "Epoch 7/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.9553 - acc: 0.5882 - val_loss: 2.7180 - val_acc: 0.6756\n",
            "Epoch 8/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.8231 - acc: 0.6274 - val_loss: 2.7360 - val_acc: 0.6766\n",
            "Epoch 9/200\n",
            "782/782 [==============================] - 70s 89ms/step - loss: 2.7910 - acc: 0.6347 - val_loss: 2.8852 - val_acc: 0.6407\n",
            "Epoch 10/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.8022 - acc: 0.6230 - val_loss: 2.9090 - val_acc: 0.6140\n",
            "Epoch 11/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.7051 - acc: 0.6431 - val_loss: 2.6535 - val_acc: 0.6663\n",
            "Epoch 12/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.5815 - acc: 0.6818 - val_loss: 2.4818 - val_acc: 0.7276\n",
            "Epoch 13/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.5554 - acc: 0.6870 - val_loss: 2.7615 - val_acc: 0.6575\n",
            "Epoch 14/200\n",
            "782/782 [==============================] - 70s 89ms/step - loss: 2.5786 - acc: 0.6706 - val_loss: 2.7371 - val_acc: 0.6461\n",
            "Epoch 15/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.5005 - acc: 0.6847 - val_loss: 2.5827 - val_acc: 0.6974\n",
            "Epoch 16/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.3813 - acc: 0.7197 - val_loss: 2.3126 - val_acc: 0.7585\n",
            "Epoch 17/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.3516 - acc: 0.7275 - val_loss: 2.3231 - val_acc: 0.7417\n",
            "Epoch 18/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.3828 - acc: 0.7076 - val_loss: 2.5983 - val_acc: 0.6841\n",
            "Epoch 19/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 2.3211 - acc: 0.7209 - val_loss: 2.1727 - val_acc: 0.7777\n",
            "Epoch 20/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.2133 - acc: 0.7510 - val_loss: 2.1004 - val_acc: 0.7983\n",
            "Epoch 21/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.1900 - acc: 0.7554 - val_loss: 2.1592 - val_acc: 0.7751\n",
            "Epoch 22/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 2.2205 - acc: 0.7354 - val_loss: 2.5795 - val_acc: 0.6729\n",
            "Epoch 23/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 2.1635 - acc: 0.7437 - val_loss: 1.9993 - val_acc: 0.8037\n",
            "Epoch 24/200\n",
            "782/782 [==============================] - 86s 110ms/step - loss: 2.0629 - acc: 0.7745 - val_loss: 1.9765 - val_acc: 0.8149\n",
            "Epoch 25/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 2.0318 - acc: 0.7844 - val_loss: 2.2510 - val_acc: 0.7397\n",
            "Epoch 26/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 2.0781 - acc: 0.7595 - val_loss: 2.3070 - val_acc: 0.7079\n",
            "Epoch 27/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 2.0346 - acc: 0.7638 - val_loss: 2.0262 - val_acc: 0.7736\n",
            "Epoch 28/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.9330 - acc: 0.7939 - val_loss: 1.8496 - val_acc: 0.8271\n",
            "Epoch 29/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.9134 - acc: 0.7975 - val_loss: 2.0005 - val_acc: 0.7838\n",
            "Epoch 30/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.9498 - acc: 0.7768 - val_loss: 2.3485 - val_acc: 0.7091\n",
            "Epoch 31/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.9129 - acc: 0.7809 - val_loss: 1.7956 - val_acc: 0.8261\n",
            "Epoch 32/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.8180 - acc: 0.8089 - val_loss: 1.7404 - val_acc: 0.8435\n",
            "Epoch 33/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.7979 - acc: 0.8132 - val_loss: 1.9099 - val_acc: 0.7964\n",
            "Epoch 34/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.8415 - acc: 0.7921 - val_loss: 2.1735 - val_acc: 0.7405\n",
            "Epoch 35/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.8064 - acc: 0.7946 - val_loss: 2.0332 - val_acc: 0.7490\n",
            "Epoch 36/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.7132 - acc: 0.8224 - val_loss: 1.6552 - val_acc: 0.8466\n",
            "Epoch 37/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.6942 - acc: 0.8265 - val_loss: 1.7781 - val_acc: 0.8162\n",
            "Epoch 38/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.7369 - acc: 0.8061 - val_loss: 1.8296 - val_acc: 0.7889\n",
            "Epoch 39/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.7094 - acc: 0.8060 - val_loss: 1.6839 - val_acc: 0.8244\n",
            "Epoch 40/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.6178 - acc: 0.8333 - val_loss: 1.5752 - val_acc: 0.8512\n",
            "Epoch 41/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.6005 - acc: 0.8365 - val_loss: 1.6375 - val_acc: 0.8332\n",
            "Epoch 42/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 1.6505 - acc: 0.8143 - val_loss: 1.8839 - val_acc: 0.7709\n",
            "Epoch 43/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.6196 - acc: 0.8189 - val_loss: 1.5668 - val_acc: 0.8431\n",
            "Epoch 44/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.5340 - acc: 0.8415 - val_loss: 1.5050 - val_acc: 0.8560\n",
            "Epoch 45/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.5139 - acc: 0.8479 - val_loss: 1.5651 - val_acc: 0.8409\n",
            "Epoch 46/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.5635 - acc: 0.8251 - val_loss: 1.7415 - val_acc: 0.7971\n",
            "Epoch 47/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.5383 - acc: 0.8261 - val_loss: 1.5388 - val_acc: 0.8307\n",
            "Epoch 48/200\n",
            "782/782 [==============================] - 87s 111ms/step - loss: 1.4548 - acc: 0.8497 - val_loss: 1.4207 - val_acc: 0.8673\n",
            "Epoch 49/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.4300 - acc: 0.8566 - val_loss: 1.5790 - val_acc: 0.8214\n",
            "Epoch 50/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.4841 - acc: 0.8336 - val_loss: 1.5569 - val_acc: 0.8197\n",
            "Epoch 51/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.4611 - acc: 0.8329 - val_loss: 1.4911 - val_acc: 0.8276\n",
            "Epoch 52/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.3768 - acc: 0.8587 - val_loss: 1.3686 - val_acc: 0.8654\n",
            "Epoch 53/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.3522 - acc: 0.8657 - val_loss: 1.3882 - val_acc: 0.8575\n",
            "Epoch 54/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.4182 - acc: 0.8388 - val_loss: 1.6581 - val_acc: 0.7921\n",
            "Epoch 55/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.3921 - acc: 0.8407 - val_loss: 1.3720 - val_acc: 0.8508\n",
            "Epoch 56/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 1.3057 - acc: 0.8661 - val_loss: 1.3018 - val_acc: 0.8743\n",
            "Epoch 57/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.2909 - acc: 0.8706 - val_loss: 1.3810 - val_acc: 0.8527\n",
            "Epoch 58/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.3442 - acc: 0.8471 - val_loss: 1.7620 - val_acc: 0.7813\n",
            "Epoch 59/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.3367 - acc: 0.8449 - val_loss: 1.3458 - val_acc: 0.8449\n",
            "Epoch 60/200\n",
            "782/782 [==============================] - 72s 91ms/step - loss: 1.2430 - acc: 0.8734 - val_loss: 1.2370 - val_acc: 0.8816\n",
            "Epoch 61/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.2198 - acc: 0.8797 - val_loss: 1.3740 - val_acc: 0.8505\n",
            "Epoch 62/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.2890 - acc: 0.8522 - val_loss: 1.4696 - val_acc: 0.8107\n",
            "Epoch 63/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.2688 - acc: 0.8528 - val_loss: 1.2425 - val_acc: 0.8653\n",
            "Epoch 64/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1847 - acc: 0.8803 - val_loss: 1.1949 - val_acc: 0.8825\n",
            "Epoch 65/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1624 - acc: 0.8842 - val_loss: 1.3381 - val_acc: 0.8456\n",
            "Epoch 66/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.2291 - acc: 0.8592 - val_loss: 1.3384 - val_acc: 0.8317\n",
            "Epoch 67/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.2105 - acc: 0.8577 - val_loss: 1.3120 - val_acc: 0.8402\n",
            "Epoch 68/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1324 - acc: 0.8827 - val_loss: 1.1614 - val_acc: 0.8825\n",
            "Epoch 69/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1093 - acc: 0.8887 - val_loss: 1.1718 - val_acc: 0.8795\n",
            "Epoch 70/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1741 - acc: 0.8635 - val_loss: 1.5783 - val_acc: 0.7794\n",
            "Epoch 71/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1582 - acc: 0.8637 - val_loss: 1.2374 - val_acc: 0.8526\n",
            "Epoch 72/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.0765 - acc: 0.8889 - val_loss: 1.0992 - val_acc: 0.8883\n",
            "Epoch 73/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.0586 - acc: 0.8928 - val_loss: 1.1589 - val_acc: 0.8697\n",
            "Epoch 74/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1256 - acc: 0.8666 - val_loss: 1.2042 - val_acc: 0.8478\n",
            "Epoch 75/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.1126 - acc: 0.8677 - val_loss: 1.1034 - val_acc: 0.8778\n",
            "Epoch 76/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.0341 - acc: 0.8926 - val_loss: 1.0444 - val_acc: 0.8927\n",
            "Epoch 77/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.0093 - acc: 0.8994 - val_loss: 1.1092 - val_acc: 0.8749\n",
            "Epoch 78/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.0737 - acc: 0.8722 - val_loss: 1.3502 - val_acc: 0.8211\n",
            "Epoch 79/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.0680 - acc: 0.8717 - val_loss: 1.1955 - val_acc: 0.8516\n",
            "Epoch 80/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9843 - acc: 0.8963 - val_loss: 1.0147 - val_acc: 0.8971\n",
            "Epoch 81/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9694 - acc: 0.9029 - val_loss: 1.1073 - val_acc: 0.8687\n",
            "Epoch 82/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 1.0337 - acc: 0.8756 - val_loss: 1.1573 - val_acc: 0.8466\n",
            "Epoch 83/200\n",
            "782/782 [==============================] - 72s 91ms/step - loss: 1.0223 - acc: 0.8750 - val_loss: 1.1069 - val_acc: 0.8606\n",
            "Epoch 84/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.9441 - acc: 0.8997 - val_loss: 0.9782 - val_acc: 0.8964\n",
            "Epoch 85/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9270 - acc: 0.9058 - val_loss: 1.1023 - val_acc: 0.8589\n",
            "Epoch 86/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9914 - acc: 0.8814 - val_loss: 1.1046 - val_acc: 0.8606\n",
            "Epoch 87/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9859 - acc: 0.8792 - val_loss: 0.9917 - val_acc: 0.8807\n",
            "Epoch 88/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9077 - acc: 0.9026 - val_loss: 0.9403 - val_acc: 0.8981\n",
            "Epoch 89/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.8880 - acc: 0.9088 - val_loss: 1.0091 - val_acc: 0.8799\n",
            "Epoch 90/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9561 - acc: 0.8825 - val_loss: 1.2493 - val_acc: 0.8145\n",
            "Epoch 91/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9514 - acc: 0.8809 - val_loss: 1.0281 - val_acc: 0.8713\n",
            "Epoch 92/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8706 - acc: 0.9072 - val_loss: 0.8944 - val_acc: 0.9014\n",
            "Epoch 93/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8535 - acc: 0.9113 - val_loss: 0.9568 - val_acc: 0.8862\n",
            "Epoch 94/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.9211 - acc: 0.8853 - val_loss: 1.1101 - val_acc: 0.8444\n",
            "Epoch 95/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.9143 - acc: 0.8864 - val_loss: 0.9883 - val_acc: 0.8745\n",
            "Epoch 96/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8339 - acc: 0.9095 - val_loss: 0.8693 - val_acc: 0.9061\n",
            "Epoch 97/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8219 - acc: 0.9128 - val_loss: 1.0171 - val_acc: 0.8649\n",
            "Epoch 98/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8834 - acc: 0.8902 - val_loss: 1.2263 - val_acc: 0.8154\n",
            "Epoch 99/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8817 - acc: 0.8872 - val_loss: 0.8766 - val_acc: 0.8966\n",
            "Epoch 100/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8057 - acc: 0.9123 - val_loss: 0.8392 - val_acc: 0.9085\n",
            "Epoch 101/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7875 - acc: 0.9172 - val_loss: 0.8916 - val_acc: 0.8931\n",
            "Epoch 102/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8583 - acc: 0.8903 - val_loss: 1.0707 - val_acc: 0.8429\n",
            "Epoch 103/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8584 - acc: 0.8876 - val_loss: 0.8981 - val_acc: 0.8860\n",
            "Epoch 104/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7741 - acc: 0.9160 - val_loss: 0.8399 - val_acc: 0.9005\n",
            "Epoch 105/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7570 - acc: 0.9207 - val_loss: 0.9090 - val_acc: 0.8786\n",
            "Epoch 106/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8260 - acc: 0.8941 - val_loss: 1.1563 - val_acc: 0.8198\n",
            "Epoch 107/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.8273 - acc: 0.8920 - val_loss: 0.9219 - val_acc: 0.8725\n",
            "Epoch 108/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7432 - acc: 0.9185 - val_loss: 0.7966 - val_acc: 0.9069\n",
            "Epoch 109/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7263 - acc: 0.9252 - val_loss: 0.8509 - val_acc: 0.8922\n",
            "Epoch 110/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7984 - acc: 0.8977 - val_loss: 0.9370 - val_acc: 0.8710\n",
            "Epoch 111/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.8001 - acc: 0.8941 - val_loss: 0.8793 - val_acc: 0.8803\n",
            "Epoch 112/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7217 - acc: 0.9208 - val_loss: 0.7878 - val_acc: 0.9050\n",
            "Epoch 113/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7042 - acc: 0.9263 - val_loss: 0.8594 - val_acc: 0.8879\n",
            "Epoch 114/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7740 - acc: 0.8976 - val_loss: 1.0027 - val_acc: 0.8422\n",
            "Epoch 115/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7773 - acc: 0.8952 - val_loss: 0.8106 - val_acc: 0.8927\n",
            "Epoch 116/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6984 - acc: 0.9226 - val_loss: 0.7432 - val_acc: 0.9140\n",
            "Epoch 117/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6774 - acc: 0.9282 - val_loss: 0.8080 - val_acc: 0.8958\n",
            "Epoch 118/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7510 - acc: 0.9018 - val_loss: 0.9440 - val_acc: 0.8572\n",
            "Epoch 119/200\n",
            "782/782 [==============================] - 88s 112ms/step - loss: 0.7549 - acc: 0.8969 - val_loss: 1.0046 - val_acc: 0.8389\n",
            "Epoch 120/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6748 - acc: 0.9239 - val_loss: 0.7325 - val_acc: 0.9114\n",
            "Epoch 121/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6574 - acc: 0.9305 - val_loss: 0.8026 - val_acc: 0.8918\n",
            "Epoch 122/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7272 - acc: 0.9016 - val_loss: 1.2894 - val_acc: 0.7918\n",
            "Epoch 123/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7311 - acc: 0.8993 - val_loss: 0.7602 - val_acc: 0.8939\n",
            "Epoch 124/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6517 - acc: 0.9277 - val_loss: 0.7340 - val_acc: 0.9047\n",
            "Epoch 125/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.6361 - acc: 0.9303 - val_loss: 0.7613 - val_acc: 0.8965\n",
            "Epoch 126/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.7059 - acc: 0.9055 - val_loss: 0.8745 - val_acc: 0.8735\n",
            "Epoch 127/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.7130 - acc: 0.9030 - val_loss: 0.7276 - val_acc: 0.9041\n",
            "Epoch 128/200\n",
            "782/782 [==============================] - 72s 91ms/step - loss: 0.6319 - acc: 0.9295 - val_loss: 0.6987 - val_acc: 0.9123\n",
            "Epoch 129/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.6149 - acc: 0.9331 - val_loss: 0.8742 - val_acc: 0.8688\n",
            "Epoch 130/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6960 - acc: 0.9050 - val_loss: 0.8385 - val_acc: 0.8738\n",
            "Epoch 131/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6929 - acc: 0.9029 - val_loss: 0.7484 - val_acc: 0.8923\n",
            "Epoch 132/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6101 - acc: 0.9304 - val_loss: 0.6918 - val_acc: 0.9131\n",
            "Epoch 133/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5972 - acc: 0.9349 - val_loss: 0.7592 - val_acc: 0.8927\n",
            "Epoch 134/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6664 - acc: 0.9087 - val_loss: 1.0312 - val_acc: 0.8363\n",
            "Epoch 135/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6781 - acc: 0.9036 - val_loss: 0.7132 - val_acc: 0.9012\n",
            "Epoch 136/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5968 - acc: 0.9317 - val_loss: 0.6609 - val_acc: 0.9183\n",
            "Epoch 137/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5784 - acc: 0.9375 - val_loss: 0.7241 - val_acc: 0.9044\n",
            "Epoch 138/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6527 - acc: 0.9102 - val_loss: 1.1849 - val_acc: 0.7941\n",
            "Epoch 139/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6612 - acc: 0.9051 - val_loss: 0.7901 - val_acc: 0.8798\n",
            "Epoch 140/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5822 - acc: 0.9317 - val_loss: 0.6683 - val_acc: 0.9109\n",
            "Epoch 141/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5641 - acc: 0.9377 - val_loss: 0.7209 - val_acc: 0.8956\n",
            "Epoch 142/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.6398 - acc: 0.9116 - val_loss: 0.9400 - val_acc: 0.8374\n",
            "Epoch 143/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6447 - acc: 0.9078 - val_loss: 0.7375 - val_acc: 0.8891\n",
            "Epoch 144/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5632 - acc: 0.9347 - val_loss: 0.6462 - val_acc: 0.9151\n",
            "Epoch 145/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5480 - acc: 0.9395 - val_loss: 0.7067 - val_acc: 0.9006\n",
            "Epoch 146/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.6273 - acc: 0.9108 - val_loss: 0.8889 - val_acc: 0.8416\n",
            "Epoch 147/200\n",
            "782/782 [==============================] - 72s 91ms/step - loss: 0.6294 - acc: 0.9094 - val_loss: 0.6735 - val_acc: 0.9002\n",
            "Epoch 148/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5487 - acc: 0.9366 - val_loss: 0.6248 - val_acc: 0.9183\n",
            "Epoch 149/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5336 - acc: 0.9412 - val_loss: 0.7564 - val_acc: 0.8825\n",
            "Epoch 150/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.6081 - acc: 0.9149 - val_loss: 1.1454 - val_acc: 0.7788\n",
            "Epoch 151/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.6171 - acc: 0.9106 - val_loss: 0.6691 - val_acc: 0.8999\n",
            "Epoch 152/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5374 - acc: 0.9373 - val_loss: 0.6179 - val_acc: 0.9176\n",
            "Epoch 153/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.5195 - acc: 0.9427 - val_loss: 0.7933 - val_acc: 0.8784\n",
            "Epoch 154/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.5931 - acc: 0.9154 - val_loss: 0.9070 - val_acc: 0.8414\n",
            "Epoch 155/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.6060 - acc: 0.9107 - val_loss: 0.6644 - val_acc: 0.8994\n",
            "Epoch 156/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5263 - acc: 0.9379 - val_loss: 0.6162 - val_acc: 0.9187\n",
            "Epoch 157/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5084 - acc: 0.9439 - val_loss: 0.7243 - val_acc: 0.8856\n",
            "Epoch 158/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5815 - acc: 0.9156 - val_loss: 1.0552 - val_acc: 0.8257\n",
            "Epoch 159/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.5870 - acc: 0.9130 - val_loss: 0.7352 - val_acc: 0.8839\n",
            "Epoch 160/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.5094 - acc: 0.9410 - val_loss: 0.6041 - val_acc: 0.9189\n",
            "Epoch 161/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.4946 - acc: 0.9452 - val_loss: 0.6715 - val_acc: 0.9018\n",
            "Epoch 162/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.5714 - acc: 0.9164 - val_loss: 0.9506 - val_acc: 0.8435\n",
            "Epoch 163/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5832 - acc: 0.9131 - val_loss: 0.6907 - val_acc: 0.8868\n",
            "Epoch 164/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.5031 - acc: 0.9411 - val_loss: 0.5833 - val_acc: 0.9198\n",
            "Epoch 165/200\n",
            "782/782 [==============================] - 71s 90ms/step - loss: 0.4825 - acc: 0.9467 - val_loss: 0.6626 - val_acc: 0.8986\n",
            "Epoch 166/200\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.5569 - acc: 0.9188 - val_loss: 0.6825 - val_acc: 0.8930\n",
            "Epoch 167/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5701 - acc: 0.9149 - val_loss: 0.7066 - val_acc: 0.8851\n",
            "Epoch 168/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.4929 - acc: 0.9415 - val_loss: 0.6023 - val_acc: 0.9139\n",
            "Epoch 169/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4798 - acc: 0.9444 - val_loss: 0.6791 - val_acc: 0.8978\n",
            "Epoch 170/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5500 - acc: 0.9196 - val_loss: 0.7182 - val_acc: 0.8856\n",
            "Epoch 171/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5600 - acc: 0.9154 - val_loss: 0.5970 - val_acc: 0.9134\n",
            "Epoch 172/200\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.4832 - acc: 0.9423 - val_loss: 0.5825 - val_acc: 0.9171\n",
            "Epoch 173/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4670 - acc: 0.9479 - val_loss: 0.7388 - val_acc: 0.8850\n",
            "Epoch 174/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5403 - acc: 0.9204 - val_loss: 0.8340 - val_acc: 0.8502\n",
            "Epoch 175/200\n",
            "782/782 [==============================] - 71s 91ms/step - loss: 0.5573 - acc: 0.9139 - val_loss: 0.6135 - val_acc: 0.9047\n",
            "Epoch 176/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4810 - acc: 0.9404 - val_loss: 0.5578 - val_acc: 0.9233\n",
            "Epoch 177/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4576 - acc: 0.9477 - val_loss: 0.6741 - val_acc: 0.8917\n",
            "Epoch 178/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5340 - acc: 0.9208 - val_loss: 0.8842 - val_acc: 0.8406\n",
            "Epoch 179/200\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.5447 - acc: 0.9168 - val_loss: 0.6400 - val_acc: 0.8961\n",
            "Epoch 180/200\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.4648 - acc: 0.9455 - val_loss: 0.5354 - val_acc: 0.9263\n",
            "Epoch 181/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4473 - acc: 0.9492 - val_loss: 0.5789 - val_acc: 0.9175\n",
            "Epoch 182/200\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.5219 - acc: 0.9233 - val_loss: 0.6610 - val_acc: 0.8913\n",
            "Epoch 183/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5400 - acc: 0.9165 - val_loss: 0.6087 - val_acc: 0.9057\n",
            "Epoch 184/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4576 - acc: 0.9448 - val_loss: 0.5541 - val_acc: 0.9223\n",
            "Epoch 185/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4397 - acc: 0.9499 - val_loss: 0.6131 - val_acc: 0.9078\n",
            "Epoch 186/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5229 - acc: 0.9214 - val_loss: 0.7796 - val_acc: 0.8572\n",
            "Epoch 187/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5335 - acc: 0.9170 - val_loss: 0.5708 - val_acc: 0.9141\n",
            "Epoch 188/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4487 - acc: 0.9464 - val_loss: 0.5316 - val_acc: 0.9231\n",
            "Epoch 189/200\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.4370 - acc: 0.9492 - val_loss: 0.6322 - val_acc: 0.9041\n",
            "Epoch 190/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5069 - acc: 0.9239 - val_loss: 0.8029 - val_acc: 0.8516\n",
            "Epoch 191/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5280 - acc: 0.9179 - val_loss: 0.5534 - val_acc: 0.9180\n",
            "Epoch 192/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4426 - acc: 0.9468 - val_loss: 0.5252 - val_acc: 0.9252\n",
            "Epoch 193/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4265 - acc: 0.9518 - val_loss: 0.6124 - val_acc: 0.9039\n",
            "Epoch 194/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5059 - acc: 0.9231 - val_loss: 0.7894 - val_acc: 0.8711\n",
            "Epoch 195/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5143 - acc: 0.9201 - val_loss: 0.5695 - val_acc: 0.9082\n",
            "Epoch 196/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4425 - acc: 0.9443 - val_loss: 0.5340 - val_acc: 0.9218\n",
            "Epoch 197/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4202 - acc: 0.9520 - val_loss: 0.5762 - val_acc: 0.9130\n",
            "Epoch 198/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4956 - acc: 0.9242 - val_loss: 0.8275 - val_acc: 0.8553\n",
            "Epoch 199/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.5150 - acc: 0.9184 - val_loss: 0.6056 - val_acc: 0.9014\n",
            "Epoch 200/200\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.4323 - acc: 0.9478 - val_loss: 0.5210 - val_acc: 0.9261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd66965e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-SIYTwWdnqWQ",
        "outputId": "eb360ada-f8dc-4e89-9318-6a9ac0870c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "cell_type": "code",
      "source": [
        "#%%matplotlibmatplotl  inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR - 'triangular' Policy\")\n",
        "plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efd63d2cac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHOXglL0DBw4cOHBwlMP/D/zIRmPDMX7lAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7efd63d2cc88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZcWydmIVhZGr",
        "outputId": "a9421885-65ac-4db1-8f7d-7fc58f008e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = tpu_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(score)\n",
        "print('Test loss: %.3f ' % (score[0]))\n",
        "print('Test accuracy: %.3f ' % (score[1]*100))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(4,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(4, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(4, 10), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7efd199d93c8> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 48.52879238128662 secs\n",
            "10000/10000 [==============================] - 77s 8ms/step\n",
            "[0.5208723777294159, 0.9265]\n",
            "Test loss: 0.521 \n",
            "Test accuracy: 92.650 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B0XKsP4J1CQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        },
        "outputId": "08ce7cee-17ba-47d8-8a16-267965fcda1a"
      },
      "cell_type": "code",
      "source": [
        "def step_decay(epoch):\n",
        "\tlrate = 0.001\n",
        "\t\n",
        "\treturn float(lrate)\n",
        "lrschedular = LearningRateScheduler(step_decay)\n",
        "\n",
        "tpu_model.fit_generator(datagen.flow(x_train, y_train,\n",
        "\t\t\t\t\t\t\t\t\t batch_size=64),\n",
        "\t\t\t\t\t\tsteps_per_epoch=782,epochs=50,validation_data=(x_test, y_test),verbose=1,callbacks=[lrschedular])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3988 - acc: 0.9590 - val_loss: 0.5156 - val_acc: 0.9285\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3991 - acc: 0.9593 - val_loss: 0.5151 - val_acc: 0.9278\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3952 - acc: 0.9612 - val_loss: 0.5149 - val_acc: 0.9270\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3965 - acc: 0.9598 - val_loss: 0.5121 - val_acc: 0.9276\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3947 - acc: 0.9607 - val_loss: 0.5125 - val_acc: 0.9269\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3951 - acc: 0.9615 - val_loss: 0.5101 - val_acc: 0.9281\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3926 - acc: 0.9608 - val_loss: 0.5142 - val_acc: 0.9268\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3955 - acc: 0.9601 - val_loss: 0.5115 - val_acc: 0.9279\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3904 - acc: 0.9617 - val_loss: 0.5088 - val_acc: 0.9282\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3901 - acc: 0.9624 - val_loss: 0.5081 - val_acc: 0.9279\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3880 - acc: 0.9625 - val_loss: 0.5094 - val_acc: 0.9288\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3892 - acc: 0.9619 - val_loss: 0.5073 - val_acc: 0.9283\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3886 - acc: 0.9625 - val_loss: 0.5081 - val_acc: 0.9288\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3853 - acc: 0.9637 - val_loss: 0.5109 - val_acc: 0.9287\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3850 - acc: 0.9638 - val_loss: 0.5112 - val_acc: 0.9272\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3818 - acc: 0.9638 - val_loss: 0.5085 - val_acc: 0.9295\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3867 - acc: 0.9635 - val_loss: 0.5091 - val_acc: 0.9298\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3830 - acc: 0.9642 - val_loss: 0.5080 - val_acc: 0.9281\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3841 - acc: 0.9642 - val_loss: 0.5078 - val_acc: 0.9291\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3835 - acc: 0.9642 - val_loss: 0.5096 - val_acc: 0.9280\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3823 - acc: 0.9633 - val_loss: 0.5075 - val_acc: 0.9300\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3814 - acc: 0.9640 - val_loss: 0.5077 - val_acc: 0.9290\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3819 - acc: 0.9642 - val_loss: 0.5083 - val_acc: 0.9291\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3783 - acc: 0.9647 - val_loss: 0.5058 - val_acc: 0.9295\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3793 - acc: 0.9648 - val_loss: 0.5098 - val_acc: 0.9287\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3782 - acc: 0.9656 - val_loss: 0.5083 - val_acc: 0.9287\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3839 - acc: 0.9642 - val_loss: 0.5091 - val_acc: 0.9290\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3788 - acc: 0.9649 - val_loss: 0.5077 - val_acc: 0.9290\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3788 - acc: 0.9644 - val_loss: 0.5077 - val_acc: 0.9288\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3772 - acc: 0.9649 - val_loss: 0.5073 - val_acc: 0.9286\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3769 - acc: 0.9657 - val_loss: 0.5078 - val_acc: 0.9296\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 65s 82ms/step - loss: 0.3771 - acc: 0.9647 - val_loss: 0.5063 - val_acc: 0.9293\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3768 - acc: 0.9664 - val_loss: 0.5086 - val_acc: 0.9297\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3768 - acc: 0.9661 - val_loss: 0.5080 - val_acc: 0.9293\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3758 - acc: 0.9656 - val_loss: 0.5087 - val_acc: 0.9295\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3751 - acc: 0.9653 - val_loss: 0.5076 - val_acc: 0.9294\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3752 - acc: 0.9671 - val_loss: 0.5081 - val_acc: 0.9293\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 65s 82ms/step - loss: 0.3710 - acc: 0.9676 - val_loss: 0.5070 - val_acc: 0.9293\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3732 - acc: 0.9660 - val_loss: 0.5052 - val_acc: 0.9313\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3738 - acc: 0.9666 - val_loss: 0.5064 - val_acc: 0.9306\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3737 - acc: 0.9658 - val_loss: 0.5090 - val_acc: 0.9279\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3735 - acc: 0.9662 - val_loss: 0.5096 - val_acc: 0.9288\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3725 - acc: 0.9667 - val_loss: 0.5070 - val_acc: 0.9309\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3732 - acc: 0.9665 - val_loss: 0.5058 - val_acc: 0.9309\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3705 - acc: 0.9673 - val_loss: 0.5056 - val_acc: 0.9315\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 64s 82ms/step - loss: 0.3714 - acc: 0.9672 - val_loss: 0.5066 - val_acc: 0.9309\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3719 - acc: 0.9677 - val_loss: 0.5059 - val_acc: 0.9301\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3715 - acc: 0.9670 - val_loss: 0.5033 - val_acc: 0.9315\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3708 - acc: 0.9664 - val_loss: 0.5044 - val_acc: 0.9314\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3672 - acc: 0.9681 - val_loss: 0.5053 - val_acc: 0.9300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd0d7b9be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "v8K1GRGc6-YO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "975a1f9b-9acd-406a-a7c1-83eda52f83d1"
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = tpu_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(score)\n",
        "print('Test loss: %.3f ' % (score[0]))\n",
        "print('Test accuracy: %.3f ' % (score[1]*100))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 12s 1ms/step\n",
            "[0.5051559878349304, 0.9303]\n",
            "Test loss: 0.505 \n",
            "Test accuracy: 93.030 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UE3lF6EH1r_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "49bf4da5-89c3-48de-a181-6fcb289abff7"
      },
      "cell_type": "code",
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "tpu_model.save_weights(\"DNST_weights_Shravan_B9_clr.h5\")\n",
        "tpu_model.save(\"DNST_model_Shravan_B9_clr.hdf5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.0\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.0\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Og56VCRh5j8V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VmSIqsez3C-o"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. Original Densenet Paper: https://arxiv.org/pdf/1608.06993\n",
        "2. https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504\n",
        "3. https://forums.fast.ai/t/training-a-model-from-scratch-cifar-10/7897\n",
        "4. https://towardsdatascience.com/densenet-2810936aeebb\n",
        "5. https://towardsdatascience.com/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686\n",
        "\n",
        "\n",
        "# Densenet implementations\n",
        "1. Original Densenet Implementation: https://github.com/liuzhuang13/DenseNet\n",
        "2. Fast.ai: http://files.fast.ai/part2/lesson13/densenet-keras.ipynb\n",
        "3. Github Users\n",
        "\ta. https://github.com/titu1994/DenseNet\n",
        "\tb. https://github.com/flyyufelix/DenseNet-Keras \n",
        "\n",
        "# LR Callbacks\n",
        "1. Cyclic Learning Rate: https://github.com/bckenstler/CLR\n",
        "2. SGDR: https://gist.github.com/t2kasa/490610116ddb0f3b664458d0e086e643\n",
        "3. SWATS: https://arxiv.org/pdf/1712.07628 (Implementation not found)\n",
        "\thttps://github.com/kweonwooj/papers/issues/76\n",
        "\thttps://www.groundai.com/project/improving-generalization-performance-by-switching-from-adam-to-sgd/\n",
        "\t"
      ]
    }
  ]
}