{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K70hAckqg0EA"
   },
   "outputs": [],
   "source": [
    "# # https://keras.io/\n",
    "# !pip install -q keras\n",
    "# import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, Activation,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint,ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uLi6C9pArQK"
   },
   "outputs": [],
   "source": [
    "#Copied code from https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py\n",
    "from tensorflow.keras.callbacks import *\n",
    "class CyclicLR(Callback):\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# # this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# # backend\n",
    "# import tensorflow as tf\n",
    "# from keras import backend as k\n",
    "\n",
    "# # Don't pre-allocate memory; allocate as-needed\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "\n",
    "# # Create a session with the above options specified.\n",
    "# k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 250\n",
    "l = 16\n",
    "num_filter = 32\n",
    "growth_rate = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2\n",
    "weight_decay = 1e-4\n",
    "dilate_rate = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB7o3zu1g6eT"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "for i in range(3):\n",
    "\t\tmean = np.mean(x_train[:, :, :, i])\n",
    "\t\tstd = np.std(x_train[:, :, :, i])\n",
    "\t\tx_train[:, :, :, i] = (x_train[:, :, :, i] - mean) / std\n",
    "\t\tx_test[:, :, :, i] = (x_test[:, :, :, i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression, weight_decay, growth_rate\n",
    "\n",
    "    temp = input\n",
    "    #concat_layers = [input]\n",
    "    for _ in range(l):\n",
    "      \n",
    "        BatchNorm = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_1_1 = Conv2D(int(growth_rate*4), (1,1), use_bias=False ,padding='same',\n",
    "                              kernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(relu)\n",
    "        \n",
    "        BatchNorm_1_1 = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(Conv2D_1_1)\n",
    "        relu_1_1 = Activation('relu')(BatchNorm_1_1)\n",
    "        Conv2D_3_3 = Conv2D(int(growth_rate), (3,3), use_bias=False ,padding='same', dilation_rate = dilate_rate,\n",
    "                              kernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(relu_1_1)\n",
    "        \n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(rate=dropout_rate)(Conv2D_3_3)\n",
    "        #concat_layers.append(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        num_filter += growth_rate\n",
    "        \n",
    "    return temp , num_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression, weight_decay\n",
    "    BatchNorm = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same',\n",
    "                              kernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(rate=dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2),strides=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression, weight_decay\n",
    "    BatchNorm = BatchNormalization(gamma_regularizer=l2(weight_decay),beta_regularizer=l2(weight_decay))(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    #AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    #flat = Flatten()(AvgPooling)\n",
    "    #output = Dense(num_classes, activation='softmax')(flat)\n",
    "    GloAvgPooling = GlobalAveragePooling2D()(relu)\n",
    "    output = Dense(num_classes, activation='softmax',\n",
    "\t\tkernel_regularizer=l2(weight_decay),bias_regularizer=l2(weight_decay))(GloAvgPooling)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "\n",
    "input = Input(shape=(img_height, img_width, channel))\n",
    "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same', dilation_rate = dilate_rate,\n",
    "\t\t\tkernel_initializer='he_uniform',kernel_regularizer=l2(weight_decay))(input)\n",
    "\n",
    "First_Block, num_filters = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, num_filters, dropout_rate)\n",
    "\n",
    "Second_Block,num_filters = add_denseblock(First_Transition, num_filters, dropout_rate)\n",
    "Second_Transition = add_transition(Second_Block, num_filters, dropout_rate)\n",
    "\n",
    "# Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "# Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block,num_filters = add_denseblock(Second_Transition,  num_filters, dropout_rate)\n",
    "output = output_layer(Last_Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 14866
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "c06b0c19-4256-4fbe-92f1-55166b32edd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 48)   1536        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 48)   192         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 48)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 12)   5184        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 44)   0           conv2d[0][0]                     \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 44)   176         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 44)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 48)   2112        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 56)   0           concatenate[0][0]                \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 56)   224         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 56)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 48)   2688        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 12)   5184        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 68)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 68)   272         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 68)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 48)   3264        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 48)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 12)   5184        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 80)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 80)   320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 48)   3840        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 48)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 48)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 12)   5184        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 92)   0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 92)   368         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 92)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 48)   4416        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 48)   192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 48)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 12)   5184        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 104)  0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 104)  416         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 104)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 48)   4992        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 48)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 12)   5184        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 116)  0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 116)  464         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 116)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 48)   5568        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 48)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 48)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 12)   5184        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 128)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 128)  512         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 48)   6144        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 48)   192         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 48)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 12)   5184        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 140)  0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 140)  560         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 140)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 48)   6720        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 48)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 48)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 12)   5184        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 152)  0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 152)  608         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 152)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 48)   7296        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 48)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 12)   5184        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 164)  0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 164)  656         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 32, 164)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 48)   7872        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 48)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 32, 48)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 12)   5184        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 176)  0           concatenate_10[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 176)  704         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 176)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 48)   8448        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 48)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 48)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 12)   5184        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 188)  0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 188)  752         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 188)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 48)   9024        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 48)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 48)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 12)   5184        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 200)  0           concatenate_12[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 200)  800         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 200)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 48)   9600        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 32, 48)   192         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 32, 48)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 12)   5184        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 32, 32, 212)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 32, 212)  848         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 32, 212)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 48)   10176       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 48)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 48)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 12)   5184        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32, 32, 12)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 32, 32, 224)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 224)  896         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 224)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 112)  25088       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 32, 32, 112)  0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 112)  0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 112)  448         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 112)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 48)   5376        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 48)   192         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 48)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 12)   5184        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 124)  0           average_pooling2d[0][0]          \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 124)  496         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 124)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 48)   5952        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 48)   192         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 48)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 12)   5184        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 136)  0           concatenate_16[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 136)  544         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 136)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 48)   6528        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 48)   192         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 48)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 12)   5184        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 148)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 148)  592         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 148)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 48)   7104        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 48)   192         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 48)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 12)   5184        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 160)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 160)  640         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 48)   7680        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 48)   192         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 48)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 12)   5184        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 172)  0           concatenate_19[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 172)  688         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 172)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 48)   8256        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 48)   192         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 48)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 16, 16, 12)   5184        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 184)  0           concatenate_20[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 184)  736         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 184)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 16, 16, 48)   8832        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 48)   192         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 48)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 16, 16, 12)   5184        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 196)  0           concatenate_21[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 16, 16, 196)  784         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 196)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 16, 16, 48)   9408        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 16, 16, 48)   192         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 48)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 16, 16, 12)   5184        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 208)  0           concatenate_22[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 16, 16, 208)  832         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 208)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 16, 16, 48)   9984        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 16, 16, 48)   192         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 16, 16, 48)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 16, 16, 12)   5184        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 220)  0           concatenate_23[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 16, 16, 220)  880         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16, 16, 220)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 16, 16, 48)   10560       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 16, 16, 48)   192         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 16, 48)   0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 16, 16, 12)   5184        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16, 16, 12)   0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 16, 16, 232)  0           concatenate_24[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 16, 16, 232)  928         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 16, 232)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 48)   11136       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 48)   192         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 48)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 12)   5184        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 16, 16, 12)   0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 244)  0           concatenate_25[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 244)  976         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 244)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 48)   11712       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 48)   192         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 48)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 12)   5184        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 16, 16, 12)   0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 256)  0           concatenate_26[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 256)  1024        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 48)   12288       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 48)   192         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 48)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 12)   5184        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 16, 16, 12)   0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 16, 16, 268)  0           concatenate_27[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 268)  1072        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 268)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 48)   12864       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 48)   192         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 48)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 12)   5184        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 16, 16, 12)   0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 16, 16, 280)  0           concatenate_28[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 280)  1120        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 280)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 48)   13440       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 48)   192         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 48)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 12)   5184        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 16, 16, 12)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 16, 16, 292)  0           concatenate_29[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 292)  1168        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 292)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 48)   14016       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 48)   192         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 48)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 12)   5184        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 16, 16, 12)   0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 304)  0           concatenate_30[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 304)  1216        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 304)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 208)  63232       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 16, 16, 208)  0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 208)    0           dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 8, 208)    832         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 8, 208)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 8, 8, 48)     9984        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 48)     192         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 8, 48)     0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 8, 12)     5184        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 8, 8, 12)     0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 220)    0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 220)    880         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 8, 220)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 8, 48)     10560       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 48)     192         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 8, 48)     0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 12)     5184        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 8, 8, 12)     0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 232)    0           concatenate_32[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 232)    928         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 8, 232)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 48)     11136       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 48)     192         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 8, 48)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 12)     5184        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 8, 8, 12)     0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 244)    0           concatenate_33[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 244)    976         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 244)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 48)     11712       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 48)     192         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 48)     0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 12)     5184        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 8, 8, 12)     0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 256)    0           concatenate_34[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 256)    1024        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 256)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 8, 48)     12288       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 8, 8, 48)     192         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 48)     0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 12)     5184        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 8, 8, 12)     0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 268)    0           concatenate_35[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 268)    1072        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 268)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 48)     12864       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 48)     192         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 48)     0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 12)     5184        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 8, 8, 12)     0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 8, 8, 280)    0           concatenate_36[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 280)    1120        concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 280)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 48)     13440       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 48)     192         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 48)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 12)     5184        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 8, 8, 12)     0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 8, 8, 292)    0           concatenate_37[0][0]             \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 292)    1168        concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 292)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 48)     14016       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 48)     192         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 48)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 12)     5184        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 8, 8, 12)     0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 8, 8, 304)    0           concatenate_38[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 304)    1216        concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 304)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 48)     14592       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 48)     192         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 48)     0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 12)     5184        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 8, 8, 12)     0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 8, 8, 316)    0           concatenate_39[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 316)    1264        concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 316)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 48)     15168       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 48)     192         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 48)     0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 12)     5184        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 8, 8, 12)     0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 8, 8, 328)    0           concatenate_40[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 328)    1312        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 328)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 48)     15744       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 48)     192         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 48)     0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 12)     5184        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 8, 8, 12)     0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 8, 8, 340)    0           concatenate_41[0][0]             \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 340)    1360        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 340)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 48)     16320       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 48)     192         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 48)     0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 12)     5184        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 8, 8, 12)     0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 8, 8, 352)    0           concatenate_42[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 352)    1408        concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 352)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 48)     16896       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 48)     192         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 48)     0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 12)     5184        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 8, 8, 12)     0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 8, 8, 364)    0           concatenate_43[0][0]             \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 364)    1456        concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 364)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 48)     17472       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 48)     192         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 48)     0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 12)     5184        activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 8, 8, 12)     0           conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 8, 8, 376)    0           concatenate_44[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 376)    1504        concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 376)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 8, 8, 48)     18048       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 48)     192         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 8, 8, 48)     0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 8, 8, 12)     5184        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 8, 8, 12)     0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 388)    0           concatenate_45[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 388)    1552        concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 8, 8, 388)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 48)     18624       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 48)     192         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 8, 8, 48)     0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 12)     5184        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 8, 8, 12)     0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 8, 8, 400)    0           concatenate_46[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 8, 8, 400)    1600        concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 8, 8, 400)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 400)          0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           4010        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 872,458\n",
      "Trainable params: 846,090\n",
      "Non-trainable params: 26,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeZElodLEG45"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "\t\t\tfeaturewise_center=False,  # set input mean to 0 over the dataset\n",
    "\t\t\tsamplewise_center=False,  # set each sample mean to 0\n",
    "\t\t\tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "\t\t\tsamplewise_std_normalization=False,  # divide each input by its std\n",
    "\t\t\tzca_whitening=False,  # apply ZCA whitening\n",
    "\t\t\trotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "\t\t\twidth_shift_range=0.16,  # randomly shift images horizontally (fraction of total width)\n",
    "\t\t\theight_shift_range=0.16,  # randomly shift images vertically (fraction of total height)\n",
    "\t\t\thorizontal_flip=True,  # randomly flip images\n",
    "\t\t\tvertical_flip=False) # randomly flip images\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0CI3oZKvqyQ"
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "\tinitial_lrate = 0.1\n",
    "\tlrate = 0.1\n",
    "\tif epoch >= 125 and epoch < 187:\n",
    "\t\tlrate = initial_lrate / 10\n",
    "\tif epoch >= 187 :\n",
    "\t\tlrate = initial_lrate / 100\n",
    "\t\n",
    "\treturn float(lrate)\n",
    "lrschedular = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqXjjnixLtKn"
   },
   "outputs": [],
   "source": [
    "lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
    "                                    cooldown=0, patience=5, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFlWLDk_m5H3"
   },
   "outputs": [],
   "source": [
    "# !rm -rf clr_callback.py*\n",
    "# !wget https://github.com/bckenstler/CLR/raw/master/clr_callback.py\n",
    "# from clr_callback import *\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular',base_lr=0.001, max_lr=0.1, step_size=782*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=Adam(),\n",
    "#               metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),#momentum=0.9,nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "w5oLYwEMqK79",
    "outputId": "c7991138-974f-4501-f33c-65af892254b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.0.210.194:8470') for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6347446215774545733)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13386831080101798575)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1825349573857696043)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12543244047257317185)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3166087646285837528)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3404024456260756328)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8732924705049378509)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2793872354322560867)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3951414271052319587)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10030443326584175246)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18029311138686683373)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 3318601942711158248)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
      "INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n"
     ]
    }
   ],
   "source": [
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(model,\n",
    "                                              strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
    "                                                  tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://'+os.environ['COLAB_TPU_ADDR'])\n",
    "                                              )\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8946
    },
    "colab_type": "code",
    "id": "crhGk7kEhXAz",
    "outputId": "824c9da7-3e52-4ffd-ae76-c8173fbb5f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(8, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(8, 10), dtype=tf.float32, name='dense_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Cloning SGD {'lr': 0.0010000000474974513, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
      "INFO:tensorflow:Remapping placeholder for input_1\n",
      "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7f0071c46ba8> []\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 118.51566290855408 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
      "INFO:tensorflow:CPU -> TPU momentum: 0.0 {0.0}\n",
      "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
      "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
      "  2/782 [..............................] - ETA: 30:14:16 - loss: 4.3993 - acc: 0.0859WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (7.719207). Check your callbacks.\n",
      "220/782 [=======>......................] - ETA: 12:37 - loss: 4.2641 - acc: 0.1475INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(2,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(2, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(2, 10), dtype=tf.float32, name='dense_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for input_1\n",
      "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7f0071c46ba8> [<tf.Variable 'tpu_139640021644736/SGD/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058cacef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058cb0400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058cb0550>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058cb0be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058c0ce48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058bd8ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058bfbdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058b6ccf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058b33ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058ad8f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058a44dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058a0dd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058a33c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00589a1ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005896bd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005890ddd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00588ffbe0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005884afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005886ccc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00587daba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00587a4ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005876d208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00586dca90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00586fffd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058649b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058634ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00585df0f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00585a6908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005850de80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058536d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058482e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058471f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058439c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00583deeb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005834bdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058313e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058337d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00582a7ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00582719b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058214ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058182d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058149cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0058175fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00580decc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00580aacc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005804cd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005803c7f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057f87d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057fa9e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057f17588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057ee5e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057e84b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057e74ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057e3cf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057d86ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057d71d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057d1ad30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057ce4eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057c4dfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057c75cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057c3ddd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057baef60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057b76ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057b1be48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057a88d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057a51e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057a77cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00579e3d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00579a9ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057951e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005793cfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005788bd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00578aff60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005781be48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00577e4e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005778bcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057778f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00576c3f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00576e4e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057657518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005761ed68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057640cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00575afc50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005757af98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00574c52b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00574b3780>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00574584a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057422e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005738df98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00573b1198>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005737c4a8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00572ebf98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005728ff60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0057258ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00571c5cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005718eda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00571b5eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005711fcc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00570e8f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005708ee10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005707ed30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056fc6ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056fecf98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056f57dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056f23da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056ec8c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056eb5d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056e7efd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056e25da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056d92c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056d5dcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056d02cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056ceeba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056cb7f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056c01240>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056beee80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056b92cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056b5cdd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056ac7f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056af1128>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056abe940>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056a26eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00569c9f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056994e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056904c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00568cdf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00568f2c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005685ce10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056826eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00567ccda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00567bbf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056707ac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056728f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056696d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005665dd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056609c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00565f0d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00565bdcf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005655fda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00564d1ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056497d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00564bfc88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00564295c0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00563f3eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00563421d0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056304c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00562cef98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056298b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056205be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056231978>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00561f8f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056163fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005610acf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00560d2e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0056041c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005600bf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005602ee80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055f9ada0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055f65e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055f09d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055ef8c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055e42f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055e67eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055dd3cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055d9afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055d44f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055d31cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055cf7e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055c9ed30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055c0df60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055bd7fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055bfbe48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055b67588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055b35e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055ad8470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055a43c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055a0efd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00559d9a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055948d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005596d4e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055935e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00558a0fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055846cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005580eda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055800ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055748c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005576cf28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00556d8d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00556a3dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005564afd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055634cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055600eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00555a5e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055514d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00554dfd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055483fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005546ddd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055438dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00553ddcc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005534df28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055314f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055338dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00552a7c50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055272d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055218400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055182be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005514cf60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055114278>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055083748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00550a9ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0055071e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054fe1f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054f83160>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054f52fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054f3bef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054ee1f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054ea8eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054e19fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054ddfc88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054d89a20>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054d74e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054d3eef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054ce3dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054c53be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054c1beb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054c3ef60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054badda0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054b76d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054b1ac50>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054a88ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054a51d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054a74dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00549e8be0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00549b0fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054957cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054940ba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005488bef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054853208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00547c4a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00547e8fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00547b0b38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005471def0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00546c60f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005468e908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005467ae80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005461ed68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00545e9e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054559f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054521c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00544c5eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00544b2dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005447ce80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005441fd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054391ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00543599b0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005437eef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00542ead30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00542b3cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005425bfd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00541c5cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054192cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00541b5d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00541247f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00540efd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0054093e80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005407f588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053fcce80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053feeb38>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053f5bef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053f27f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053eedac8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053e5bd68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053e02d30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053dcceb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053db7fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053d5ecf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053d28dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053c98f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053c60ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053c03e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053bf0d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053bbbe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053b61cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053aced30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053a96ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053abae80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053a29fd0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00539f7d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053999d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053904e48>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00538cee10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00538f1cf8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053862f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005382cf98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00537cfe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00537be518>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f0053709d68>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f005372ccc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f00536f2cf8>]\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 92.3878448009491 secs\n",
      "781/782 [============================>.] - ETA: 0s - loss: 4.0816 - acc: 0.2185INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(8, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(8, 10), dtype=tf.float32, name='dense_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Cloning SGD {'lr': 0.0010000000474974513, 'momentum': 0.0, 'decay': 0.0, 'nesterov': False}\n",
      "INFO:tensorflow:Remapping placeholder for input_1\n",
      "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7f0036e4b0b8> []\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 67.62958073616028 secs\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(2,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(2, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(2, 10), dtype=tf.float32, name='dense_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for input_1\n",
      "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7f0036e4b0b8> []\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 41.96291160583496 secs\n",
      "782/782 [==============================] - 673s 861ms/step - loss: 4.0814 - acc: 0.2186 - val_loss: 3.8263 - val_acc: 0.2981\n",
      "Epoch 2/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 3.7774 - acc: 0.3261 - val_loss: 3.5928 - val_acc: 0.3860\n",
      "Epoch 3/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 3.6086 - acc: 0.3929 - val_loss: 3.4863 - val_acc: 0.4381\n",
      "Epoch 4/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 3.4516 - acc: 0.4473 - val_loss: 3.2951 - val_acc: 0.5034\n",
      "Epoch 5/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 3.3191 - acc: 0.4903 - val_loss: 3.8765 - val_acc: 0.4525\n",
      "Epoch 6/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 3.1792 - acc: 0.5302 - val_loss: 2.9911 - val_acc: 0.5952\n",
      "Epoch 7/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 3.0525 - acc: 0.5656 - val_loss: 3.2700 - val_acc: 0.5598\n",
      "Epoch 8/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.9405 - acc: 0.5967 - val_loss: 3.3078 - val_acc: 0.5304\n",
      "Epoch 9/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 2.8166 - acc: 0.6190 - val_loss: 2.6770 - val_acc: 0.6660\n",
      "Epoch 10/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.7103 - acc: 0.6459 - val_loss: 2.5477 - val_acc: 0.7001\n",
      "Epoch 11/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 2.6084 - acc: 0.6692 - val_loss: 2.4732 - val_acc: 0.7226\n",
      "Epoch 12/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 2.5212 - acc: 0.6895 - val_loss: 2.5301 - val_acc: 0.7090\n",
      "Epoch 13/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.4678 - acc: 0.7013 - val_loss: 2.2538 - val_acc: 0.7771\n",
      "Epoch 14/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.4101 - acc: 0.7183 - val_loss: 2.3226 - val_acc: 0.7589\n",
      "Epoch 15/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 2.3527 - acc: 0.7315 - val_loss: 2.2258 - val_acc: 0.7862\n",
      "Epoch 16/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.3255 - acc: 0.7388 - val_loss: 2.2151 - val_acc: 0.7889\n",
      "Epoch 17/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 2.3156 - acc: 0.7431 - val_loss: 2.2033 - val_acc: 0.7887\n",
      "Epoch 18/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 2.3169 - acc: 0.7419 - val_loss: 2.3965 - val_acc: 0.7410\n",
      "Epoch 19/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 2.3129 - acc: 0.7361 - val_loss: 2.2766 - val_acc: 0.7609\n",
      "Epoch 20/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.3065 - acc: 0.7335 - val_loss: 2.8229 - val_acc: 0.6547\n",
      "Epoch 21/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 2.2846 - acc: 0.7345 - val_loss: 2.7959 - val_acc: 0.6520\n",
      "Epoch 22/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.2468 - acc: 0.7378 - val_loss: 2.1414 - val_acc: 0.7807\n",
      "Epoch 23/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.2112 - acc: 0.7423 - val_loss: 2.3695 - val_acc: 0.7273\n",
      "Epoch 24/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 2.1623 - acc: 0.7433 - val_loss: 2.3860 - val_acc: 0.7313\n",
      "Epoch 25/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 2.1002 - acc: 0.7550 - val_loss: 2.4763 - val_acc: 0.6943\n",
      "Epoch 26/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 2.0247 - acc: 0.7711 - val_loss: 1.9853 - val_acc: 0.8015\n",
      "Epoch 27/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.9593 - acc: 0.7823 - val_loss: 1.8904 - val_acc: 0.8135\n",
      "Epoch 28/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.9099 - acc: 0.7914 - val_loss: 1.7877 - val_acc: 0.8386\n",
      "Epoch 29/240\n",
      "782/782 [==============================] - 75s 96ms/step - loss: 1.8549 - acc: 0.8043 - val_loss: 1.7604 - val_acc: 0.8420\n",
      "Epoch 30/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.8159 - acc: 0.8136 - val_loss: 1.7517 - val_acc: 0.8468\n",
      "Epoch 31/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.7756 - acc: 0.8239 - val_loss: 1.7651 - val_acc: 0.8392\n",
      "Epoch 32/240\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 1.7572 - acc: 0.8299 - val_loss: 1.7140 - val_acc: 0.8516\n",
      "Epoch 33/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 1.7494 - acc: 0.8304 - val_loss: 1.7315 - val_acc: 0.8479\n",
      "Epoch 34/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 1.7541 - acc: 0.8284 - val_loss: 1.8196 - val_acc: 0.8257\n",
      "Epoch 35/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.7655 - acc: 0.8209 - val_loss: 1.8082 - val_acc: 0.8301\n",
      "Epoch 36/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.7646 - acc: 0.8161 - val_loss: 1.7871 - val_acc: 0.8248\n",
      "Epoch 37/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.7540 - acc: 0.8129 - val_loss: 1.8475 - val_acc: 0.8125\n",
      "Epoch 38/240\n",
      "782/782 [==============================] - 75s 95ms/step - loss: 1.7450 - acc: 0.8097 - val_loss: 1.8531 - val_acc: 0.7954\n",
      "Epoch 39/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.7368 - acc: 0.8057 - val_loss: 1.8263 - val_acc: 0.7839\n",
      "Epoch 40/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.7151 - acc: 0.8053 - val_loss: 1.7526 - val_acc: 0.8068\n",
      "Epoch 41/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.6750 - acc: 0.8081 - val_loss: 1.7399 - val_acc: 0.8193\n",
      "Epoch 42/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.6160 - acc: 0.8188 - val_loss: 1.6085 - val_acc: 0.8294\n",
      "Epoch 43/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.5579 - acc: 0.8313 - val_loss: 1.5896 - val_acc: 0.8369\n",
      "Epoch 44/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.5213 - acc: 0.8402 - val_loss: 1.5670 - val_acc: 0.8410\n",
      "Epoch 45/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.4815 - acc: 0.8464 - val_loss: 1.6541 - val_acc: 0.8193\n",
      "Epoch 46/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.4445 - acc: 0.8567 - val_loss: 1.4302 - val_acc: 0.8683\n",
      "Epoch 47/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.4120 - acc: 0.8677 - val_loss: 1.4361 - val_acc: 0.8641\n",
      "Epoch 48/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.3869 - acc: 0.8715 - val_loss: 1.4056 - val_acc: 0.8728\n",
      "Epoch 49/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 1.3830 - acc: 0.8725 - val_loss: 1.3774 - val_acc: 0.8796\n",
      "Epoch 50/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.3891 - acc: 0.8689 - val_loss: 1.4247 - val_acc: 0.8686\n",
      "Epoch 51/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.4003 - acc: 0.8626 - val_loss: 1.4271 - val_acc: 0.8663\n",
      "Epoch 52/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.4065 - acc: 0.8575 - val_loss: 1.4898 - val_acc: 0.8407\n",
      "Epoch 53/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.4158 - acc: 0.8480 - val_loss: 1.4579 - val_acc: 0.8486\n",
      "Epoch 54/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.4098 - acc: 0.8446 - val_loss: 1.6178 - val_acc: 0.7986\n",
      "Epoch 55/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.4144 - acc: 0.8389 - val_loss: 1.6551 - val_acc: 0.7826\n",
      "Epoch 56/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.4044 - acc: 0.8344 - val_loss: 1.8884 - val_acc: 0.7478\n",
      "Epoch 57/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 1.3710 - acc: 0.8404 - val_loss: 1.3462 - val_acc: 0.8555\n",
      "Epoch 58/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.3242 - acc: 0.8489 - val_loss: 1.3321 - val_acc: 0.8513\n",
      "Epoch 59/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.2784 - acc: 0.8584 - val_loss: 1.3661 - val_acc: 0.8440\n",
      "Epoch 60/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 1.2441 - acc: 0.8657 - val_loss: 1.2641 - val_acc: 0.8688\n",
      "Epoch 61/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 1.2050 - acc: 0.8752 - val_loss: 1.2024 - val_acc: 0.8812\n",
      "Epoch 62/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.1792 - acc: 0.8822 - val_loss: 1.2154 - val_acc: 0.8714\n",
      "Epoch 63/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.1473 - acc: 0.8899 - val_loss: 1.1821 - val_acc: 0.8846\n",
      "Epoch 64/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.1272 - acc: 0.8984 - val_loss: 1.1505 - val_acc: 0.8929\n",
      "Epoch 65/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.1228 - acc: 0.8980 - val_loss: 1.1447 - val_acc: 0.8938\n",
      "Epoch 66/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.1373 - acc: 0.8915 - val_loss: 1.2375 - val_acc: 0.8683\n",
      "Epoch 67/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.1372 - acc: 0.8909 - val_loss: 1.2167 - val_acc: 0.8712\n",
      "Epoch 68/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.1525 - acc: 0.8810 - val_loss: 1.4585 - val_acc: 0.8166\n",
      "Epoch 69/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 1.1645 - acc: 0.8751 - val_loss: 1.2099 - val_acc: 0.8688\n",
      "Epoch 70/240\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 1.1669 - acc: 0.8676 - val_loss: 1.4306 - val_acc: 0.8223\n",
      "Epoch 71/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.1736 - acc: 0.8629 - val_loss: 1.2473 - val_acc: 0.8429\n",
      "Epoch 72/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 1.1734 - acc: 0.8564 - val_loss: 1.6180 - val_acc: 0.7622\n",
      "Epoch 73/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 1.1541 - acc: 0.8585 - val_loss: 1.3333 - val_acc: 0.8249\n",
      "Epoch 74/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 1.1087 - acc: 0.8704 - val_loss: 1.1738 - val_acc: 0.8582\n",
      "Epoch 75/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.0802 - acc: 0.8752 - val_loss: 1.1152 - val_acc: 0.8740\n",
      "Epoch 76/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.0454 - acc: 0.8840 - val_loss: 1.1579 - val_acc: 0.8614\n",
      "Epoch 77/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.0071 - acc: 0.8930 - val_loss: 1.0162 - val_acc: 0.8906\n",
      "Epoch 78/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.9803 - acc: 0.8994 - val_loss: 1.0508 - val_acc: 0.8851\n",
      "Epoch 79/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.9499 - acc: 0.9080 - val_loss: 1.0392 - val_acc: 0.8849\n",
      "Epoch 80/240\n",
      "782/782 [==============================] - 75s 95ms/step - loss: 0.9319 - acc: 0.9135 - val_loss: 0.9842 - val_acc: 0.9017\n",
      "Epoch 81/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.9265 - acc: 0.9165 - val_loss: 0.9675 - val_acc: 0.9040\n",
      "Epoch 82/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.9341 - acc: 0.9110 - val_loss: 0.9891 - val_acc: 0.8975\n",
      "Epoch 83/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.9514 - acc: 0.9046 - val_loss: 1.1337 - val_acc: 0.8564\n",
      "Epoch 84/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.9559 - acc: 0.9001 - val_loss: 1.0127 - val_acc: 0.8923\n",
      "Epoch 85/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.9743 - acc: 0.8916 - val_loss: 1.1295 - val_acc: 0.8609\n",
      "Epoch 86/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.9909 - acc: 0.8829 - val_loss: 1.1597 - val_acc: 0.8475\n",
      "Epoch 87/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.9958 - acc: 0.8777 - val_loss: 1.2016 - val_acc: 0.8311\n",
      "Epoch 88/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 1.0063 - acc: 0.8702 - val_loss: 1.2262 - val_acc: 0.8261\n",
      "Epoch 89/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.9835 - acc: 0.8741 - val_loss: 1.2108 - val_acc: 0.8225\n",
      "Epoch 90/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.9498 - acc: 0.8844 - val_loss: 1.0221 - val_acc: 0.8693\n",
      "Epoch 91/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.9142 - acc: 0.8913 - val_loss: 0.9845 - val_acc: 0.8788\n",
      "Epoch 92/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.8899 - acc: 0.8982 - val_loss: 0.9670 - val_acc: 0.8800\n",
      "Epoch 93/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.8579 - acc: 0.9044 - val_loss: 0.9104 - val_acc: 0.8991\n",
      "Epoch 94/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.8249 - acc: 0.9153 - val_loss: 0.9198 - val_acc: 0.8898\n",
      "Epoch 95/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.8007 - acc: 0.9220 - val_loss: 0.8581 - val_acc: 0.9086\n",
      "Epoch 96/240\n",
      "782/782 [==============================] - 72s 91ms/step - loss: 0.7777 - acc: 0.9283 - val_loss: 0.8524 - val_acc: 0.9095\n",
      "Epoch 97/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.7726 - acc: 0.9319 - val_loss: 0.8537 - val_acc: 0.9103\n",
      "Epoch 98/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.7814 - acc: 0.9260 - val_loss: 0.8441 - val_acc: 0.9095\n",
      "Epoch 99/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.7993 - acc: 0.9196 - val_loss: 0.9428 - val_acc: 0.8797\n",
      "Epoch 100/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.8142 - acc: 0.9114 - val_loss: 1.1455 - val_acc: 0.8378\n",
      "Epoch 101/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.8367 - acc: 0.9025 - val_loss: 0.8846 - val_acc: 0.8944\n",
      "Epoch 102/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.8470 - acc: 0.8960 - val_loss: 1.0148 - val_acc: 0.8624\n",
      "Epoch 103/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.8608 - acc: 0.8872 - val_loss: 0.9303 - val_acc: 0.8740\n",
      "Epoch 104/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.8729 - acc: 0.8816 - val_loss: 0.9657 - val_acc: 0.8750\n",
      "Epoch 105/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.8634 - acc: 0.8824 - val_loss: 0.9854 - val_acc: 0.8582\n",
      "Epoch 106/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.8268 - acc: 0.8932 - val_loss: 1.0604 - val_acc: 0.8398\n",
      "Epoch 107/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.7938 - acc: 0.9012 - val_loss: 0.8990 - val_acc: 0.8802\n",
      "Epoch 108/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.7689 - acc: 0.9076 - val_loss: 0.8035 - val_acc: 0.8986\n",
      "Epoch 109/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.7347 - acc: 0.9186 - val_loss: 0.7850 - val_acc: 0.9081\n",
      "Epoch 110/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.7087 - acc: 0.9245 - val_loss: 0.7716 - val_acc: 0.9104\n",
      "Epoch 111/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.6857 - acc: 0.9325 - val_loss: 0.8029 - val_acc: 0.9030\n",
      "Epoch 112/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6658 - acc: 0.9374 - val_loss: 0.7508 - val_acc: 0.9151\n",
      "Epoch 113/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.6605 - acc: 0.9411 - val_loss: 0.7430 - val_acc: 0.9163\n",
      "Epoch 114/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6712 - acc: 0.9342 - val_loss: 0.7599 - val_acc: 0.9108\n",
      "Epoch 115/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.6836 - acc: 0.9302 - val_loss: 0.7960 - val_acc: 0.9016\n",
      "Epoch 116/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.7061 - acc: 0.9220 - val_loss: 0.9447 - val_acc: 0.8596\n",
      "Epoch 117/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.7224 - acc: 0.9130 - val_loss: 0.9584 - val_acc: 0.8569\n",
      "Epoch 118/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.7458 - acc: 0.9048 - val_loss: 0.9209 - val_acc: 0.8625\n",
      "Epoch 119/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.7570 - acc: 0.8964 - val_loss: 0.8662 - val_acc: 0.8766\n",
      "Epoch 120/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.7719 - acc: 0.8895 - val_loss: 0.9105 - val_acc: 0.8576\n",
      "Epoch 121/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.7653 - acc: 0.8903 - val_loss: 0.8100 - val_acc: 0.8862\n",
      "Epoch 122/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.7302 - acc: 0.9002 - val_loss: 0.8709 - val_acc: 0.8673\n",
      "Epoch 123/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.7057 - acc: 0.9079 - val_loss: 0.8236 - val_acc: 0.8789\n",
      "Epoch 124/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.6752 - acc: 0.9176 - val_loss: 0.7311 - val_acc: 0.9100\n",
      "Epoch 125/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.6447 - acc: 0.9268 - val_loss: 0.7386 - val_acc: 0.8998\n",
      "Epoch 126/240\n",
      "782/782 [==============================] - 72s 91ms/step - loss: 0.6221 - acc: 0.9323 - val_loss: 0.7233 - val_acc: 0.9060\n",
      "Epoch 127/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.6001 - acc: 0.9384 - val_loss: 0.6728 - val_acc: 0.9191\n",
      "Epoch 128/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5780 - acc: 0.9463 - val_loss: 0.6668 - val_acc: 0.9211\n",
      "Epoch 129/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5758 - acc: 0.9462 - val_loss: 0.6843 - val_acc: 0.9156\n",
      "Epoch 130/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5812 - acc: 0.9441 - val_loss: 0.6871 - val_acc: 0.9182\n",
      "Epoch 131/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.5974 - acc: 0.9360 - val_loss: 0.6884 - val_acc: 0.9181\n",
      "Epoch 132/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.6173 - acc: 0.9297 - val_loss: 0.7383 - val_acc: 0.9020\n",
      "Epoch 133/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6385 - acc: 0.9202 - val_loss: 0.8324 - val_acc: 0.8832\n",
      "Epoch 134/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.6616 - acc: 0.9090 - val_loss: 0.8500 - val_acc: 0.8715\n",
      "Epoch 135/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.6865 - acc: 0.9004 - val_loss: 0.7845 - val_acc: 0.8844\n",
      "Epoch 136/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.6970 - acc: 0.8959 - val_loss: 0.8905 - val_acc: 0.8517\n",
      "Epoch 137/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6915 - acc: 0.8968 - val_loss: 0.8720 - val_acc: 0.8566\n",
      "Epoch 138/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6630 - acc: 0.9053 - val_loss: 0.7009 - val_acc: 0.9019\n",
      "Epoch 139/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.6301 - acc: 0.9167 - val_loss: 0.7113 - val_acc: 0.8975\n",
      "Epoch 140/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.6074 - acc: 0.9223 - val_loss: 0.7110 - val_acc: 0.9015\n",
      "Epoch 141/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5778 - acc: 0.9307 - val_loss: 0.6599 - val_acc: 0.9142\n",
      "Epoch 142/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.5581 - acc: 0.9368 - val_loss: 0.6895 - val_acc: 0.9035\n",
      "Epoch 143/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5308 - acc: 0.9456 - val_loss: 0.6430 - val_acc: 0.9180\n",
      "Epoch 144/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5126 - acc: 0.9507 - val_loss: 0.6224 - val_acc: 0.9248\n",
      "Epoch 145/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5039 - acc: 0.9548 - val_loss: 0.6337 - val_acc: 0.9180\n",
      "Epoch 146/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5167 - acc: 0.9494 - val_loss: 0.6493 - val_acc: 0.9166\n",
      "Epoch 147/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5331 - acc: 0.9423 - val_loss: 0.6831 - val_acc: 0.9067\n",
      "Epoch 148/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.5487 - acc: 0.9350 - val_loss: 0.7218 - val_acc: 0.8941\n",
      "Epoch 149/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.5726 - acc: 0.9266 - val_loss: 0.8797 - val_acc: 0.8567\n",
      "Epoch 150/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.6026 - acc: 0.9151 - val_loss: 0.7383 - val_acc: 0.8837\n",
      "Epoch 151/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.6179 - acc: 0.9075 - val_loss: 0.7044 - val_acc: 0.8912\n",
      "Epoch 152/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6397 - acc: 0.9003 - val_loss: 0.9577 - val_acc: 0.8311\n",
      "Epoch 153/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.6324 - acc: 0.9024 - val_loss: 0.7325 - val_acc: 0.8829\n",
      "Epoch 154/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.6091 - acc: 0.9096 - val_loss: 0.7518 - val_acc: 0.8775\n",
      "Epoch 155/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.5763 - acc: 0.9202 - val_loss: 0.7249 - val_acc: 0.8848\n",
      "Epoch 156/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.5529 - acc: 0.9267 - val_loss: 0.6817 - val_acc: 0.8986\n",
      "Epoch 157/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.5308 - acc: 0.9332 - val_loss: 0.6575 - val_acc: 0.9031\n",
      "Epoch 158/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5003 - acc: 0.9429 - val_loss: 0.5957 - val_acc: 0.9192\n",
      "Epoch 159/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4802 - acc: 0.9493 - val_loss: 0.5638 - val_acc: 0.9281\n",
      "Epoch 160/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.4625 - acc: 0.9551 - val_loss: 0.5703 - val_acc: 0.9271\n",
      "Epoch 161/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4582 - acc: 0.9575 - val_loss: 0.5729 - val_acc: 0.9259\n",
      "Epoch 162/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4647 - acc: 0.9541 - val_loss: 0.5916 - val_acc: 0.9199\n",
      "Epoch 163/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4795 - acc: 0.9479 - val_loss: 0.6140 - val_acc: 0.9126\n",
      "Epoch 164/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5035 - acc: 0.9384 - val_loss: 0.6745 - val_acc: 0.8972\n",
      "Epoch 165/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5259 - acc: 0.9299 - val_loss: 0.6277 - val_acc: 0.9102\n",
      "Epoch 166/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.5473 - acc: 0.9221 - val_loss: 0.7587 - val_acc: 0.8737\n",
      "Epoch 167/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5759 - acc: 0.9114 - val_loss: 0.6592 - val_acc: 0.8982\n",
      "Epoch 168/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.6029 - acc: 0.9029 - val_loss: 0.9059 - val_acc: 0.8427\n",
      "Epoch 169/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5961 - acc: 0.9042 - val_loss: 0.6295 - val_acc: 0.9013\n",
      "Epoch 170/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.5619 - acc: 0.9151 - val_loss: 0.6791 - val_acc: 0.8869\n",
      "Epoch 171/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5408 - acc: 0.9226 - val_loss: 0.6310 - val_acc: 0.9007\n",
      "Epoch 172/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5131 - acc: 0.9295 - val_loss: 0.6452 - val_acc: 0.9003\n",
      "Epoch 173/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.4849 - acc: 0.9401 - val_loss: 0.5863 - val_acc: 0.9141\n",
      "Epoch 174/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4606 - acc: 0.9478 - val_loss: 0.5640 - val_acc: 0.9225\n",
      "Epoch 175/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4392 - acc: 0.9530 - val_loss: 0.5346 - val_acc: 0.9310\n",
      "Epoch 176/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.4285 - acc: 0.9577 - val_loss: 0.5328 - val_acc: 0.9306\n",
      "Epoch 177/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.4179 - acc: 0.9618 - val_loss: 0.5412 - val_acc: 0.9285\n",
      "Epoch 178/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4252 - acc: 0.9567 - val_loss: 0.5693 - val_acc: 0.9203\n",
      "Epoch 179/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4422 - acc: 0.9518 - val_loss: 0.5886 - val_acc: 0.9136\n",
      "Epoch 180/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4611 - acc: 0.9433 - val_loss: 0.5760 - val_acc: 0.9161\n",
      "Epoch 181/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4829 - acc: 0.9353 - val_loss: 0.8603 - val_acc: 0.8517\n",
      "Epoch 182/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5112 - acc: 0.9249 - val_loss: 0.7460 - val_acc: 0.8732\n",
      "Epoch 183/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5438 - acc: 0.9144 - val_loss: 0.7926 - val_acc: 0.8559\n",
      "Epoch 184/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5649 - acc: 0.9068 - val_loss: 0.6359 - val_acc: 0.8933\n",
      "Epoch 185/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.5589 - acc: 0.9088 - val_loss: 0.7728 - val_acc: 0.8624\n",
      "Epoch 186/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5340 - acc: 0.9186 - val_loss: 0.5735 - val_acc: 0.9103\n",
      "Epoch 187/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5074 - acc: 0.9241 - val_loss: 0.6239 - val_acc: 0.9030\n",
      "Epoch 188/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4784 - acc: 0.9354 - val_loss: 0.5947 - val_acc: 0.9097\n",
      "Epoch 189/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4570 - acc: 0.9411 - val_loss: 0.6049 - val_acc: 0.9050\n",
      "Epoch 190/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.4305 - acc: 0.9498 - val_loss: 0.5680 - val_acc: 0.9169\n",
      "Epoch 191/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4082 - acc: 0.9578 - val_loss: 0.5538 - val_acc: 0.9223\n",
      "Epoch 192/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.3929 - acc: 0.9630 - val_loss: 0.5299 - val_acc: 0.9264\n",
      "Epoch 193/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.3867 - acc: 0.9643 - val_loss: 0.5295 - val_acc: 0.9278\n",
      "Epoch 194/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.3950 - acc: 0.9610 - val_loss: 0.5509 - val_acc: 0.9208\n",
      "Epoch 195/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4085 - acc: 0.9549 - val_loss: 0.5853 - val_acc: 0.9112\n",
      "Epoch 196/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4353 - acc: 0.9449 - val_loss: 0.5654 - val_acc: 0.9104\n",
      "Epoch 197/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.4572 - acc: 0.9369 - val_loss: 0.7717 - val_acc: 0.8662\n",
      "Epoch 198/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.4906 - acc: 0.9254 - val_loss: 0.7598 - val_acc: 0.8663\n",
      "Epoch 199/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.5110 - acc: 0.9184 - val_loss: 1.1930 - val_acc: 0.7800\n",
      "Epoch 200/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5370 - acc: 0.9104 - val_loss: 0.9266 - val_acc: 0.8286\n",
      "Epoch 201/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.5425 - acc: 0.9083 - val_loss: 0.6955 - val_acc: 0.8752\n",
      "Epoch 202/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.5078 - acc: 0.9184 - val_loss: 0.6032 - val_acc: 0.9037\n",
      "Epoch 203/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4821 - acc: 0.9277 - val_loss: 0.6770 - val_acc: 0.8826\n",
      "Epoch 204/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.4573 - acc: 0.9353 - val_loss: 0.5714 - val_acc: 0.9116\n",
      "Epoch 205/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4284 - acc: 0.9460 - val_loss: 0.5255 - val_acc: 0.9242\n",
      "Epoch 206/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4080 - acc: 0.9520 - val_loss: 0.5080 - val_acc: 0.9312\n",
      "Epoch 207/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3884 - acc: 0.9582 - val_loss: 0.5045 - val_acc: 0.9307\n",
      "Epoch 208/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.3689 - acc: 0.9656 - val_loss: 0.4941 - val_acc: 0.9335\n",
      "Epoch 209/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3679 - acc: 0.9651 - val_loss: 0.4988 - val_acc: 0.9329\n",
      "Epoch 210/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3723 - acc: 0.9627 - val_loss: 0.5468 - val_acc: 0.9213\n",
      "Epoch 211/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3875 - acc: 0.9571 - val_loss: 0.5635 - val_acc: 0.9135\n",
      "Epoch 212/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4067 - acc: 0.9507 - val_loss: 0.6596 - val_acc: 0.8925\n",
      "Epoch 213/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4342 - acc: 0.9385 - val_loss: 0.5896 - val_acc: 0.9023\n",
      "Epoch 214/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4661 - acc: 0.9284 - val_loss: 0.6816 - val_acc: 0.8823\n",
      "Epoch 215/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.4976 - acc: 0.9179 - val_loss: 0.7941 - val_acc: 0.8569\n",
      "Epoch 216/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.5199 - acc: 0.9106 - val_loss: 0.8576 - val_acc: 0.8448\n",
      "Epoch 217/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.5137 - acc: 0.9127 - val_loss: 0.5400 - val_acc: 0.9179\n",
      "Epoch 218/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.4891 - acc: 0.9217 - val_loss: 0.5838 - val_acc: 0.9066\n",
      "Epoch 219/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.4660 - acc: 0.9297 - val_loss: 0.6171 - val_acc: 0.8941\n",
      "Epoch 220/240\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.4405 - acc: 0.9378 - val_loss: 0.5933 - val_acc: 0.9031\n",
      "Epoch 221/240\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.4122 - acc: 0.9462 - val_loss: 0.5164 - val_acc: 0.9231\n",
      "Epoch 222/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3902 - acc: 0.9538 - val_loss: 0.5172 - val_acc: 0.9220\n",
      "Epoch 223/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.3725 - acc: 0.9588 - val_loss: 0.4985 - val_acc: 0.9268\n",
      "Epoch 224/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.3551 - acc: 0.9651 - val_loss: 0.4796 - val_acc: 0.9326\n",
      "Epoch 225/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3491 - acc: 0.9678 - val_loss: 0.4811 - val_acc: 0.9315\n",
      "Epoch 226/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3588 - acc: 0.9637 - val_loss: 0.5063 - val_acc: 0.9255\n",
      "Epoch 227/240\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.3718 - acc: 0.9586 - val_loss: 0.5082 - val_acc: 0.9258\n",
      "Epoch 228/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3916 - acc: 0.9515 - val_loss: 0.5095 - val_acc: 0.9214\n",
      "Epoch 229/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4167 - acc: 0.9433 - val_loss: 0.6407 - val_acc: 0.8893\n",
      "Epoch 230/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4432 - acc: 0.9324 - val_loss: 0.8322 - val_acc: 0.8509\n",
      "Epoch 231/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4715 - acc: 0.9235 - val_loss: 0.5818 - val_acc: 0.9001\n",
      "Epoch 232/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4999 - acc: 0.9134 - val_loss: 0.8103 - val_acc: 0.8527\n",
      "Epoch 233/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4948 - acc: 0.9160 - val_loss: 0.5846 - val_acc: 0.8997\n",
      "Epoch 234/240\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4679 - acc: 0.9245 - val_loss: 0.5417 - val_acc: 0.9130\n",
      "Epoch 235/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.4492 - acc: 0.9308 - val_loss: 0.5568 - val_acc: 0.9100\n",
      "Epoch 236/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.4206 - acc: 0.9419 - val_loss: 0.4946 - val_acc: 0.9229\n",
      "Epoch 237/240\n",
      "782/782 [==============================] - 72s 93ms/step - loss: 0.3957 - acc: 0.9484 - val_loss: 0.5212 - val_acc: 0.9218\n",
      "Epoch 238/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3712 - acc: 0.9578 - val_loss: 0.4857 - val_acc: 0.9283\n",
      "Epoch 239/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3540 - acc: 0.9631 - val_loss: 0.4720 - val_acc: 0.9319\n",
      "Epoch 240/240\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3426 - acc: 0.9671 - val_loss: 0.4672 - val_acc: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0083dd2b38>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=50,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(x_test, y_test),\n",
    "#                     callbacks=[clr_triangular])\n",
    "\n",
    "tpu_model.fit_generator(datagen.flow(x_train, y_train,\n",
    "\t\t\t\t\t\t\t\t\t batch_size=64),\n",
    "\t\t\t\t\t\tsteps_per_epoch=782,epochs=240,validation_data=(x_test, y_test),verbose=1,callbacks=[clr_triangular])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "-SIYTwWdnqWQ",
    "outputId": "f777b5f9-6840-46ac-9e52-b942135ec321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0073ca68d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0073ca6898>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%matplotlibmatplotl  inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "aefbadef-b9b6-4389-e06d-886b4cb30158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(4,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(4, 32, 32, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(4, 10), dtype=tf.float32, name='dense_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for input_1\n",
      "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7f0036e4b0b8> []\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 45.530253648757935 secs\n",
      "10000/10000 [==============================] - 74s 7ms/step\n",
      "[0.4677452404499054, 0.9329]\n",
      "Test loss: 0.468 \n",
      "Test accuracy: 93.290 \n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = tpu_model.evaluate(x_test, y_test, verbose=1)\n",
    "print(score)\n",
    "print('Test loss: %.3f ' % (score[0]))\n",
    "print('Test accuracy: %.3f ' % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "15a38511-b700-444b-bd26-cd8886e0db4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Copying TPU weights to the CPU\n",
      "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
      "INFO:tensorflow:TPU -> CPU momentum: 0.0\n",
      "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
      "INFO:tensorflow:TPU -> CPU nesterov: False\n",
      "WARNING:tensorflow:Cannot update non-variable config: nesterov\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "tpu_model.save_weights(\"DNST_weights_Shravan_B9_1M.h5\")\n",
    "tpu_model.save(\"DNST_model_Shravan_B9_1M.hdf5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og56VCRh5j8V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNST_CIFAR10_Shravan_B9_CLR_93.29.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
